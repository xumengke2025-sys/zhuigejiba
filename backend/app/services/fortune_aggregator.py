"""
å‘½è¿æ€»ç»“å®˜æœåŠ¡
è´Ÿè´£èšåˆ 49 ä½å¤§å¸ˆçš„æ¨æ¼”ç»“æœï¼Œæå–å…±è¯†ã€å†²çªåŠå›¾è°±æ•°æ®
"""

import json
import concurrent.futures
import random
import re
import datetime
from typing import Dict, Any, List, Optional
from ..utils.llm_client import LLMClient
from ..utils.logger import get_logger

logger = get_logger('wannian.fortune_aggregator')

GRAPH_PROMPT = """
# Role: å‘½è¿æ¶æ„å¸ˆ
# Task: åŸºäº49ä½å¤§å¸ˆé¢„æµ‹ï¼Œæ„å»ºæœªæ¥ {{future_years}} å¹´çš„â€œèµ›åšå¤©æœºå›¾è°±â€JSONã€‚

# Requirements:
1. **ç»“æ„åŒ–åˆ†ç±» (Crucial)**ï¼šæ¯å¹´æ¯ä¸ªç»´åº¦ï¼ˆäº‹ä¸š/è´¢å¯Œ/æƒ…æ„Ÿ/å¥åº·ï¼‰å¿…é¡»åŒ…å«ä»¥ä¸‹ä¸‰ç§ç±»å‹çš„èŠ‚ç‚¹ï¼š
   - **consensus (å…±è¯†)**ï¼šè‡³å°‘ 60% å¤§å¸ˆè¾¾æˆçš„æ ¸å¿ƒå…±è¯†ã€‚`master_name` å›ºå®šä¸º "ä¼—å¸ˆå…±è¯†"ã€‚æ¯ä¸ªç»´åº¦æ¯å¹´ 1 ä¸ªã€‚
   - **unique (ç‹¬ç‰¹è§‚ç‚¹)**ï¼šæŸä½å¤§å¸ˆæå‡ºçš„ä¸ä¼—ä¸åŒçš„æ·±åˆ»æ´å¯Ÿã€‚`master_name` å¿…é¡»æ˜¯å…·ä½“å¤§å¸ˆåã€‚**æ¯ä¸ªç»´åº¦æ¯å¹´ 1-3 ä¸ª**ã€‚
   - **variable (å‘½ç†å˜æ•°)**ï¼šé¢„æµ‹ä¸­çš„ä¸ç¡®å®šé¡¹ã€å†²çªç‚¹æˆ–è½¬æŠ˜å¥‘æœºã€‚`master_name` å¿…é¡»æ˜¯å…·ä½“å¤§å¸ˆåã€‚**æ¯ä¸ªç»´åº¦æ¯å¹´ 1-2 ä¸ª**ã€‚

2. **å…±è¯†èŠ‚ç‚¹å†…å®¹è¦æ±‚ (Critical)**ï¼š
   - å…±è¯†èŠ‚ç‚¹çš„ `description` å¿…é¡»çœŸæ­£æ±‡æ€»å¤šä½å¤§å¸ˆçš„å…±åŒè§‚ç‚¹ï¼Œä¸æ˜¯ç®€å•å¤åˆ¶æŸä¸€ä½å¤§å¸ˆçš„è¨€è®º
   - å¿…é¡»åˆ—ä¸¾â€œå¤šä½å¤§å¸ˆä¸€è‡´è®¤ä¸º...â€çš„å…±åŒè§‚ç‚¹ï¼Œå¹¶è¯´æ˜ä¸ºä»€ä¹ˆè¿™æ˜¯å…±è¯†
   - æ ¼å¼ï¼šâ€œå¤šä½å¤§å¸ˆä¸€è‡´è®¤ä¸º[å…·ä½“è§‚ç‚¹]ã€‚å…¶ä¸­å¢¨ç„ä»å‘¨æ˜“è§’åº¦æŒ‡å‡º...ï¼Œäº‘æ¾å±…å£«åˆ™ä»ç´«å¾®æ–¹é¢...ï¼Œè¿™äº›è§‚ç‚¹åœ¨[å…·ä½“æ–¹é¢]ä¸Šé«˜åº¦å¥‘åˆã€‚â€

3. **èŠ‚ç‚¹æ ‡é¢˜è¦æ±‚ (Critical - æå…¶é‡è¦)**ï¼š
   - `name` å­—æ®µå¿…é¡»æ˜¯ **2-5 ä¸ªå­—çš„å‘½ç†ç‰¹å¾æ ‡é¢˜**ï¼Œç”¨äºåœ¨å›¾è°±ä¸­ç›´æ¥å±•ç¤º
   - å¿…é¡»æ˜¯å…·ä½“çš„å‘½ç†ç‰¹å¾è¯ï¼Œå¦‚ï¼šâ€œæ™‹å‡æœºé‡â€ã€â€œè´µäººç›¸åŠ©â€ã€â€œæ¡ƒèŠ±æ—ºç››â€ã€â€œè‚‚èƒƒè°ƒå…»â€ã€â€œå¶è´¢å¯æœŸâ€ã€â€œå°äººé˜²èŒƒâ€
   - ç»å¯¹ç¦æ­¢æŠ½è±¡è¡¨è¾¾ï¼šâ€œäº‹ä¸šå…±è¯†â€ã€â€œè´¢å¯Œå˜åŒ–â€ã€â€œè¿åŠ¿èµ°å‘â€ã€â€œå¥åº·çŠ¶å†µâ€ã€â€œæƒ…æ„Ÿè¿åŠ¿â€
   - æ ‡é¢˜ç¤ºä¾‹ï¼š
     - äº‹ä¸šï¼šâ€œæ™‹å‡æœºé‡â€ã€â€œè´µäººç›¸åŠ©â€ã€â€œå°äººé˜²èŒƒâ€ã€â€œè½¬å‹å¥‘æœºâ€ã€â€œäº‹ä¸šç¨³å®šâ€ã€â€œå­¦ä¸šè¿›æ­¥â€
     - è´¢å¯Œï¼šâ€œæ­£è´¢ç¨³å¥â€ã€â€œå¶è´¢å¯æœŸâ€ã€â€œç ´è´¢é¢„è­¦â€ã€â€œæŠ•èµ„è°¨æ…â€ã€â€œå¼€æºèŠ‚æµâ€
     - æƒ…æ„Ÿï¼šâ€œæ¡ƒèŠ±æ—ºç››â€ã€â€œå©šå§»ç¨³å›ºâ€ã€â€œæ„Ÿæƒ…è­¦ç¤ºâ€ã€â€œå®¶åº­å’Œç¦â€ã€â€œå­å¥³ç¼˜æ—ºâ€
     - å¥åº·ï¼šâ€œè‚‚èƒƒè°ƒå…»â€ã€â€œæ„å¤–é˜²èŒƒâ€ã€â€œå¿ƒç†è°ƒæ•´â€ã€â€œä½“è´¨è°ƒç†â€ã€â€œå¹³å®‰é¡ºé‚â€

4. **èŠ‚ç‚¹æè¿°è¦æ±‚ (Critical)**ï¼š
   - æ¯ä¸ªèŠ‚ç‚¹çš„ `description` å¿…é¡»åŒ…å« 200-350 å­—çš„è¯¦ç»†åˆ†æ
   - **å¯¹äº unique/variable èŠ‚ç‚¹ï¼ˆæå…¶é‡è¦ï¼‰**ï¼šå¿…é¡»ä¿ç•™è¯¥å¤§å¸ˆçš„**åŸå§‹è¯­é£å’Œä¸“ä¸šæœ¯è¯­**ï¼ˆå¦‚â€œå®˜æ€æ··æ‚â€ã€â€œå¤©å…‹åœ°å†²â€ç­‰ï¼‰ï¼Œ**ç»å¯¹ç¦æ­¢**å°†å…¶æ”¹å†™ä¸ºé€šç”¨çš„â€œè¿åŠ¿å˜å¥½/å˜åâ€åºŸè¯ã€‚
   - æè¿°ä¸­**ç¦æ­¢**å‡ºç°â€œè¿™ä½å¤§å¸ˆè®¤ä¸ºâ€ã€â€œæ ¹æ®é¢„æµ‹â€ç­‰åºŸè¯å¥—è¯ï¼Œç›´æ¥é™ˆè¿°è§‚ç‚¹ã€‚
   - å¿…é¡»åŒ…å«ï¼šå…·ä½“æ—¶é—´èŠ‚ç‚¹ã€äº‹ä»¶æè¿°ã€åŸå› åˆ†æã€åº”å¯¹å»ºè®®
   - è®©ç”¨æˆ·èƒ½å¤Ÿæ¸…æ™°ç†è§£è¿™ä¸ªè§‚ç‚¹æ˜¯ä»€ä¹ˆã€ä¸ºä»€ä¹ˆã€æ€ä¹ˆåŠ

5. **æº¯æº**ï¼šé™¤å…±è¯†èŠ‚ç‚¹å¤–ï¼Œå¿…é¡»ç²¾å‡†æŒ‡æ˜è§‚ç‚¹å‡ºè‡ªå“ªä½å¤§å¸ˆã€‚

6. **å…³è”æ€§**ï¼šå¿…é¡»æ„å»ºèŠ‚ç‚¹é—´çš„ `edges`ã€‚å…³ç³»ç±»å‹åŒ…æ‹¬ï¼š
   - "å› æœ" (Causal): ä¸€ä¸ªäº‹ä»¶å¯¼è‡´å¦ä¸€ä¸ªã€‚
   - "å¯¹å†²" (Conflict): ä¸¤ä¸ªç»´åº¦é—´çš„çŸ›ç›¾æˆ–è§‚ç‚¹å†²çªã€‚
   - "äº’è¡¥" (Complement): äº’ç›¸ä¿ƒè¿›ã€‚
   - "æ—¶åº" (Sequence): è·¨å¹´ä»½çš„å½±å“ã€‚

7. **49ä½å¤§å¸ˆæ„è§èšåˆé€»è¾‘ (é‡è¦)**ï¼š
   - ä½ ä¼šæ”¶åˆ° 49 ä½å¤§å¸ˆçš„æ¨æ¼”æ‘˜è¦æ–‡æœ¬ï¼Œæ¯æ®µä»¥ `--- ã€å¤§å¸ˆåã€‘ ---` å¼€å¤´ï¼Œåé¢æ˜¯è¯¥å¤§å¸ˆå¯¹æœªæ¥è‹¥å¹²å¹´çš„äº‹ä¸š/è´¢å¯Œ/æƒ…æ„Ÿ/å¥åº·åˆ†æã€‚
   - è¯·æŒ‰ã€Œå¹´ä»½ Ã— ç»´åº¦ã€ï¼ˆå¦‚ 2026å¹´-äº‹ä¸šï¼‰å¯¹æ‰€æœ‰å†…å®¹è¿›è¡Œåˆ†ç»„ï¼Œåœ¨åŒä¸€ç»„å†…å®Œæˆä»¥ä¸‹æ­¥éª¤ï¼š
     1) ä¸ºæ¯ä½å¤§å¸ˆåœ¨è¯¥ç»„å†…å®¹ä¸­æå– 2-4 ä¸ª **æ ¸å¿ƒå…³é”®è¯**ï¼Œæ¯ä¸ªä¸º 2-5 ä¸ªæ±‰å­—çš„å…·ä½“çŸ­è¯­ï¼ˆå¦‚â€œæ™‹å‡æœºé‡â€â€œè´µäººæ‰¶æŒâ€â€œå°äººé˜²èŒƒâ€â€œè½¬å‹å…³å£â€ç­‰ï¼‰ï¼Œç¦æ­¢ä½¿ç”¨æŠ½è±¡è¯æˆ–æ–­å¥ç‰‡æ®µã€‚
     2) æŒ‰è¯­ä¹‰å°†å«ä¹‰ç›¸è¿‘çš„å…³é”®è¯èšç±»ä¸ºä¸»é¢˜ç°‡ï¼Œç»Ÿè®¡æ¯ä¸ªä¸»é¢˜ç°‡è¢«å¤šå°‘ä¸åŒå¤§å¸ˆæåŠã€‚
     3) é¢‘æ¬¡è¶³å¤Ÿé«˜ã€ä¸”è¡¨è¾¾æ–¹å‘åŸºæœ¬ä¸€è‡´çš„ä¸»é¢˜ç°‡ï¼Œç”Ÿæˆ `consensus` å…±è¯†èŠ‚ç‚¹ï¼›æè¿°ä¸­è¦ç»¼åˆå¤šä½å¤§å¸ˆçš„è§‚ç‚¹ï¼Œè€Œä¸æ˜¯ç®€å•å¤åˆ¶å•ä¸€å¤§å¸ˆåŸæ–‡ã€‚
     4) åªæœ‰å°‘æ•°å¤§å¸ˆæåŠï¼Œæˆ–åœ¨ç«‹åœºä¸Šæ˜æ˜¾åç¦»å…±è¯†ä½†å…·æœ‰å‚è€ƒä»·å€¼çš„ä¸»é¢˜ç°‡ï¼Œç”Ÿæˆ `unique` ç‹¬ç‰¹è§‚ç‚¹èŠ‚ç‚¹ï¼Œ**å¿…é¡»ä¿ç•™å…¶ç‹¬ç‰¹çš„é¢„æµ‹ç»†èŠ‚å’Œè¯­æ°”ï¼Œä¸è¦å°†å…¶åŒè´¨åŒ–**ã€‚
     5) åœ¨åŒä¸€ä¸»é¢˜ä¸Šå­˜åœ¨â€œæœºä¼š vs é£é™©â€æ˜æ˜¾åˆ†æ­§ï¼Œæˆ–æ–‡æœ¬ä¸­å‡ºç°â€œå¦‚æœâ€¦åˆ™â€¦â€ã€â€œä¸€æ—¦â€¦â€ç­‰æ¡ä»¶è½¬æŠ˜ï¼Œæˆ–ä¸åŒå¹´ä»½ä¹‹é—´å‡ºç°æ˜æ˜¾èµ°å‘æ”¹å˜çš„ï¼Œç”Ÿæˆ `variable` å‘½ç†å˜æ•°èŠ‚ç‚¹ï¼Œå¼ºè°ƒå…¶ä¸ç¡®å®šæ€§ä¸è½¬æŠ˜æ€§ã€‚
     6) ä¸ºæ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆç¬¦åˆæœ¬æç¤ºä¸­æ ‡é¢˜/æè¿°è¦æ±‚çš„ `name` å’Œ `description` å­—æ®µï¼Œä½¿ç”¨æˆ·ä¸€çœ¼å°±èƒ½ç†è§£è¿™ä¸ªèŠ‚ç‚¹çš„æ ¸å¿ƒå«ä¹‰ã€‚

# Output JSON Structure:
{
  "graph_data": {
    "nodes": [{"id": "n1", "properties": {"name": "2-5å­—å‘½ç†ç‰¹å¾æ ‡é¢˜", "time": "2026å¹´", "description": "200-350å­—è¯¦ç»†åˆ†æ...", "master_name": "ä¼—å¸ˆå…±è¯†|å…·ä½“å¤§å¸ˆå", "school_source": "..", "type": "consensus|unique|variable", "impact": 1-10, "dimension": "career|wealth|emotion|health"}}],
    "edges": [{"source": "n1", "target": "n2", "label": "å…³ç³»æè¿°", "type": "causal|conflict|complement|sequence"}]
  },
  "consensus": ["å…±è¯†ç‚¹1", "å…±è¯†ç‚¹2"], 
  "conflicts": ["å†²çªç‚¹1", "å†²çªç‚¹2"]
}
"""

SUMMARY_PROMPT = """
# Role: å‘½è¿æ€»ç»“å®˜
# Task: åŸºäº49ä½å¤§å¸ˆé¢„æµ‹ï¼Œæ’°å†™ä¸€ä»½å…¨æ¡ˆè‡´è¾ Markdownã€‚
# Requirements:
1. **ä¸€è‡´æ€§**ï¼šä¸¥ç¦ç¼–é€ ã€‚ç¡®ä¿æ¯ä¸ªè§‚ç‚¹ä¸äº‹å®é€»è¾‘å»åˆã€‚
2. **ç»“æ„**ï¼šæŒ‰å¹´ä»½åŠç»´åº¦(äº‹ä¸š/è´¢å¯Œ/æƒ…æ„Ÿ/å¥åº·)ç»„ç»‡ã€‚
3. **é£æ ¼**ï¼šä¼˜ç¾è‡ªç„¶ï¼Œå°†ç»“æ„åŒ–é€»è¾‘è½¬åŒ–ä¸ºæ„Ÿæ€§è§£è¯»ã€‚
# Output Format:
## ğŸ”® æ ¸å¿ƒå…±è¯†ä¸ç‹¬ç‰¹ä¿¡å·
### ğŸŒŒ æ ¸å¿ƒå…±è¯†
...
### âš¡ ç‹¬ç‰¹ä¿¡å·
...
## ğŸ“… æœªæ¥ {{future_years}} å¹´æ—¶ç©ºæ¨æ¼”è¡¨
### 2026å¹´ (ä¸™åˆ)
#### ğŸ’¼ äº‹ä¸š
- **ä¼—å¸ˆå…±è¯†**ï¼š...
- **ç‹¬ç‰¹è§†è§’**ï¼š...
...
"""

class FortuneAggregator:
    """å‘½è¿æ€»ç»“å®˜"""
    
    def __init__(self, llm_client: Optional[LLMClient] = None):
        self.llm = llm_client or LLMClient()
    
    def aggregate_reports(
        self, 
        user_data: Dict[str, Any], 
        reports: Dict[str, Dict[str, Any]],
        on_progress: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        å¹¶è¡ŒèšåˆæŠ¥å‘Šï¼šåŒæ—¶ç”Ÿæˆå›¾è°±å’Œæ–‡æœ¬ï¼Œå¤§å¹…æå‡é€Ÿåº¦
        """
        if on_progress:
            on_progress(92, "æ­£åœ¨æ‹¨åŠ¨æ˜Ÿç›˜ï¼Œèƒå– 49 ä½å¤§å¸ˆæ¨æ¼”ç²¾è¦...")
            
        future_years = user_data.get("future_years", 3)
        
        reports_text_preview = ""
        full_reports_text = ""
        for agent_id, data in reports.items():
            # å¢åŠ é¢„è§ˆé•¿åº¦ä»¥ä¿ç•™æ›´å¤šç‹¬ç‰¹è§‚ç‚¹ï¼Œé˜²æ­¢ LLM åªæœ‰å¼€å¤´å¥—è¯
            content_preview = data['content'][:800] + "..." if len(data['content']) > 800 else data['content']
            reports_text_preview += f"\n--- ã€{data['name']}ã€‘ ---\n{content_preview}\n"
            full_reports_text += f"\n--- ã€{data['name']}ã€‘ ---\n{data['content']}\n"
        
        user_context = f"ç”¨æˆ·ä¿¡æ¯: {json.dumps(user_data, ensure_ascii=False)}\næ¨æ¼”æ‘˜è¦: {reports_text_preview}"

        reports_list = list(reports.values())
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œä¸¤ä¸ªè€—æ—¶çš„ LLM ä»»åŠ¡
        # æ³¨æ„ï¼šä¸ä½¿ç”¨ with è¯­å¥ï¼Œä»¥ä¾¿åœ¨è¶…æ—¶å‘ç”Ÿæ—¶èƒ½é€šè¿‡ shutdown(wait=False) å¼ºåˆ¶ä¸ç­‰å¾…åƒµå°¸çº¿ç¨‹
        executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)
        try:
            if on_progress:
                on_progress(94, "æ­£åœ¨æ ¡å‡†å¤©æ˜Ÿæ–¹ä½ï¼Œå‡èšæ—¶ç©ºå¤©æœºå›¾è°±...")
            
            # ä»»åŠ¡ 1: ç”Ÿæˆå›¾è°± JSON
            graph_future = executor.submit(
                self.llm.chat_json, 
                [{"role": "system", "content": GRAPH_PROMPT.replace("{{future_years}}", str(future_years))},
                 {"role": "user", "content": user_context}],
                temperature=0.3, use_boost=True
            )
            
            # ä»»åŠ¡ 2: ç”Ÿæˆæ€»ç»“æ–‡æœ¬
            summary_future = executor.submit(
                self.llm.chat,
                [{"role": "system", "content": SUMMARY_PROMPT.replace("{{future_years}}", str(future_years))},
                 {"role": "user", "content": f"è¯·åŸºäºå¤§å¸ˆæ¨æ¼”æ‘˜è¦æ’°å†™æŠ¥å‘Šï¼š\n{user_context}"}],
                temperature=0.7, use_boost=True
            )

            # è·å–å›¾è°±ç»“æœ - å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 180s
            try:
                graph_result = graph_future.result(timeout=180)
                logger.info("å›¾è°± JSON ç”ŸæˆæˆåŠŸ")
            except concurrent.futures.TimeoutError:
                logger.error("å›¾è°±ç”Ÿæˆä»»åŠ¡è¶…æ—¶ (180s)ï¼Œæ­£åœ¨å¯åŠ¨ Fallback æœºåˆ¶...")
                graph_result = self._generate_fallback_graph(reports_list, future_years)
            except Exception as e:
                logger.error(f"å›¾è°±ç”Ÿæˆå‘ç”Ÿå¼‚å¸¸: {str(e)}ï¼Œæ­£åœ¨å¯åŠ¨ Fallback æœºåˆ¶...")
                graph_result = self._generate_fallback_graph(reports_list, future_years)

            if on_progress:
                on_progress(97, "å¤©æœºæ­£åœ¨å‡èšï¼Œæ­£åœ¨ç¼–æ’°å…¨æ¡ˆè‡´è¾...")

            # è·å–æ€»ç»“ç»“æœ - å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 180s
            try:
                summary_text = summary_future.result(timeout=180)
                logger.info("æ€»ç»“æ–‡æœ¬ç”ŸæˆæˆåŠŸ")
            except concurrent.futures.TimeoutError:
                logger.error("æ€»ç»“æ–‡æœ¬ç”Ÿæˆè¶…æ—¶ (180s)ï¼Œä½¿ç”¨é»˜è®¤å ä½æ–‡æœ¬")
                summary_text = "ï¼ˆå¤©æœºè¿è¡Œç¨æ˜¾è¿Ÿæ»ï¼Œç”±äºæ¨æ¼”è§„æ¨¡å·¨å¤§ï¼Œæ€»ç»“ç”Ÿæˆè¶…æ—¶ã€‚è¯·ç›´æ¥æŸ¥é˜…ä¸‹æ–¹è¯¦ç»†å›¾è°±ä¸å¤§å¸ˆæŠ¥å‘Šï¼‰"
            except Exception as e:
                logger.error(f"æ€»ç»“ç”Ÿæˆå‘ç”Ÿå¼‚å¸¸: {str(e)}")
                summary_text = "ï¼ˆå¤©æœºè¿è¡Œç¨æ˜¾è¿Ÿæ»ï¼Œè¯·ç›´æ¥æŸ¥é˜…ä¸‹æ–¹è¯¦ç»†å›¾è°±ä¸å¤§å¸ˆæŠ¥å‘Šï¼‰"
        
        finally:
            # å…³é”®ä¿®å¤ï¼šä¸å†ç­‰å¾…çº¿ç¨‹ç»“æŸï¼Œé˜²æ­¢å›  LLM å®¢æˆ·ç«¯æŒ‚æ­»å¯¼è‡´ä¸»çº¿ç¨‹æ°¸ä¹…é˜»å¡
            executor.shutdown(wait=False)

        # æ•°æ®æ¸…æ´—ä¸è¡¥å…¨ - ä¼ å…¥ reports_list ç”¨äºæ™ºèƒ½æŠ“å–æè¿°
        if not isinstance(graph_result, dict) or not graph_result.get("graph_data", {}).get("nodes"):
            logger.warning("å›¾è°±ç”Ÿæˆç»“æœå¼‚å¸¸æˆ–ä¸ºç©ºï¼Œå¼ºåˆ¶ä½¿ç”¨ fallback ç”Ÿæˆ")
            graph_result = self._generate_fallback_graph(reports_list, future_years)
        
        # è°ƒè¯•ï¼šæ£€æŸ¥ graph_result å†…å®¹
        logger.info(f"æ¸…æ´—å‰å›¾è°±èŠ‚ç‚¹æ•°: {len(graph_result.get('graph_data', {}).get('nodes', []))}")
        logger.info(f"æ¸…æ´—å‰å›¾è°±è¾¹æ•°: {len(graph_result.get('graph_data', {}).get('edges', []))}")
            
        # é¢„å¤„ç†æŠ¥å‘Šåˆ—è¡¨ï¼Œæå–æ®µè½ï¼Œå¤§å¹…æå‡ fallback å’Œ sanitize çš„é€Ÿåº¦
        preprocessed_reports = []
        for r in reports_list:
            content = r.get('content', '')
            paras = [p.strip() for p in re.split(r'[\nã€‚ï¼ï¼Ÿ]', content) if p.strip()]
            preprocessed_reports.append({
                "name": r.get('name', 'æœªçŸ¥å¤§å¸ˆ'),
                "paragraphs": paras
            })

        graph_result = self._sanitize_result(graph_result, future_years, preprocessed_reports)
        
        final_result = graph_result
        final_result["summary_text"] = summary_text
        
        # è°ƒè¯•æ—¥å¿—ï¼šç¡®è®¤è¿”å›æ•°æ®ç»“æ„
        logger.info("="*60)
        logger.info("æœ€ç»ˆè¿”å›æ•°æ®ç»“æ„æ£€æŸ¥ï¼š")
        logger.info(f"final_result keys: {list(final_result.keys())}")
        logger.info(f"summary_text é•¿åº¦: {len(summary_text) if summary_text else 0}")
        logger.info(f"graph_data æ˜¯å¦å­˜åœ¨: {('graph_data' in final_result)}")
        if 'graph_data' in final_result:
            logger.info(f"graph_data keys: {list(final_result['graph_data'].keys())}")
            logger.info(f"nodes æ•°é‡: {len(final_result['graph_data'].get('nodes', []))}")
            logger.info(f"edges æ•°é‡: {len(final_result['graph_data'].get('edges', []))}")
            # è¾“å‡ºå‰3ä¸ªèŠ‚ç‚¹çš„æ‘˜è¦
            for i, node in enumerate(final_result['graph_data'].get('nodes', [])[:3]):
                logger.info(f"èŠ‚ç‚¹ {i+1}: id={node.get('id')}, name={node.get('properties', {}).get('name')}, type={node.get('properties', {}).get('type')}")
        logger.info("="*60)
        
        if on_progress:
            on_progress(100, "å¤©æœºå·²ç°ï¼Œå…¨æ¡ˆæ¨æ¼”ç¼–æ’°å®Œæˆ")
            
        return final_result

    def _extract_rich_description(self, preprocessed_reports: List[Dict[str, Any]], dimension: str, year: str, exclude_texts: List[str] = None) -> tuple:
        """ä»ä¸Šä¸‹æ–‡ä¸­æŠ“å–å†…å®¹ä¸°å¯Œçš„æè¿°æ–‡æœ¬åŠå¯¹åº”çš„å¤§å¸ˆå§“å
        
        Args:
            preprocessed_reports: é¢„å¤„ç†åçš„æŠ¥å‘Šåˆ—è¡¨ [{'name': '...', 'paragraphs': [...]}, ...]
            exclude_texts: å·²ä½¿ç”¨çš„æè¿°åˆ—è¡¨ï¼Œé¿å…é‡å¤æå–ç›¸åŒå†…å®¹
        """
        if exclude_texts is None:
            exclude_texts = []
            
        keywords = {
            "career": ["äº‹ä¸š", "å·¥ä½œ", "æ™‹å‡", "èŒåœº", "åˆ›ä¸š", "åå£°", "å®˜", "å­¦ä¸š", "èŒä½", "å‡è¿", "ä¸šç»©"],
            "wealth": ["è´¢å¯Œ", "é‡‘é’±", "æŠ•èµ„", "æ”¶ç›Š", "ç ´è´¢", "è´¢è¿", "é‡‘", "åˆ©", "ç†è´¢", "èµ„äº§", "æ”¶å…¥"],
            "emotion": ["æ„Ÿæƒ…", "å©šå§»", "æ‹çˆ±", "æ¡ƒèŠ±", "ä¼´ä¾£", "å®¶åº­", "æƒ…", "ç¼˜", "çˆ±æƒ…", "é…å¶", "å§»ç¼˜"],
            "health": ["å¥åº·", "èº«ä½“", "ç–¾ç—…", "å…»ç”Ÿ", "å¹³å®‰", "ç–¾", "å®‰", "ä½“è´¨", "è°ƒå…»", "åŒ»"]
        }
        
        target_keys = keywords.get(dimension, [])
        candidates = []
        
        for report in preprocessed_reports:
            master_name = report.get('name', 'æœªçŸ¥å¤§å¸ˆ')
            paragraphs = report.get('paragraphs', [])
            
            for para in paragraphs:
                if len(para) < 30: continue
                
                # æ£€æŸ¥æ˜¯å¦å·²è¢«ä½¿ç”¨
                if any(para[:50] in used for used in exclude_texts):
                    continue
                
                score = 0
                has_year = year[:4] in para if year else False
                key_count = sum(1 for k in target_keys if k in para)
                
                if has_year:
                    score += 50
                score += key_count * 10
                score += min(len(para), 200) // 10
                
                if score > 0:
                    candidates.append((score, para, master_name))
        
        if candidates:
            candidates.sort(key=lambda x: -x[0])
            return candidates[0][1][:250], candidates[0][2]
        
        return "", "å¤§å¸ˆå…±é¸£"
    
    def _extract_multiple_descriptions(self, preprocessed_reports: List[Dict[str, Any]], dimension: str, year: str, count: int) -> List[tuple]:
        """ä»æŠ¥å‘Šä¸­æå–å¤šä¸ªä¸åŒçš„æè¿°"""
        results = []
        exclude_texts = []
        
        for _ in range(count * 2):
            desc, master = self._extract_rich_description(preprocessed_reports, dimension, year, exclude_texts)
            if desc and desc not in exclude_texts:
                results.append((desc, master))
                exclude_texts.append(desc)
                if len(results) >= count:
                    break
        
        return results

    def _synthesize_consensus_description(self, preprocessed_reports: List[Dict[str, Any]], dimension: str, year: str) -> str:
        """æ±‡æ€»å¤šä½å¤§å¸ˆçš„å…±åŒè§‚ç‚¹"""
        dim_names = {"career": "äº‹ä¸š", "wealth": "è´¢å¯Œ", "emotion": "æƒ…æ„Ÿ", "health": "å¥åº·"}
        dim_name = dim_names.get(dimension, "è¿åŠ¿")
        
        all_opinions = self._extract_multiple_descriptions(preprocessed_reports, dimension, year, 6)
        
        if len(all_opinions) < 2:
            # å¦‚æœåªæœ‰ä¸€ä¸ªè§‚ç‚¹ï¼Œç›´æ¥è¿”å›
            if all_opinions:
                return f"ã€å¤šä½å¤§å¸ˆå…±è¯†ã€‘å…³äº{year}{dim_name}è¿åŠ¿ï¼Œ{all_opinions[0][0]}"
            return f"ã€å¤šä½å¤§å¸ˆå…±è¯†ã€‘å…³äº{year}{dim_name}è¿åŠ¿ï¼Œå¤šä½å¤§å¸ˆç»™å‡ºäº†ä¸€è‡´çš„å»ºè®®ã€‚"
        
        # æ„å»ºæ±‡æ€»æ€§æè¿°
        masters_mentioned = []
        key_points = []
        
        for desc, master in all_opinions[:4]:  # å–å‰4ä¸ªè§‚ç‚¹
            if master not in masters_mentioned:
                masters_mentioned.append(master)
            # æå–å…³é”®çŸ­è¯­ï¼ˆå‰60å­—ï¼‰
            key_point = desc[:60].rstrip("ã€‚ï¼Œï¼Œï¼ï¼Ÿ") if len(desc) > 60 else desc
            key_points.append(key_point)
        
        # æ„å»ºå…±è¯†æè¿°
        masters_str = "ã€".join(masters_mentioned[:3])
        if len(masters_mentioned) > 3:
            masters_str += "ç­‰"
        
        consensus_desc = f"ã€å¤šä½å¤§å¸ˆå…±è¯†ã€‘å…³äº{year}{dim_name}è¿åŠ¿ï¼Œ{masters_str}å¤šä½å¤§å¸ˆè¾¾æˆäº†é«˜åº¦å…±è¯†ã€‚"
        
        # æ·»åŠ å„ä½å¤§å¸ˆçš„è§‚ç‚¹æ‘˜è¦
        for i, (desc, master) in enumerate(all_opinions[:3]):
            point = desc[:80].rstrip("ã€‚ï¼Œï¼Œï¼ï¼Ÿ") if len(desc) > 80 else desc
            if i == 0:
                consensus_desc += f" å…¶ä¸­{master}æŒ‡å‡ºï¼š{point}"
            else:
                consensus_desc += f"ï¼›{master}åˆ™è®¤ä¸ºï¼š{point}"
        
        consensus_desc += "ã€‚"
        
        # æ·»åŠ ç»¼åˆå»ºè®®
        consensus_desc += f" ç»¼åˆæ¥çœ‹ï¼Œ{year}çš„{dim_name}è¿åŠ¿éœ€è¦é‡ç‚¹å…³æ³¨ä»¥ä¸Šå‡ ç‚¹ï¼Œåˆç†è§„åˆ’ã€æŠŠæ¡æ—¶æœºã€‚"
        
        return consensus_desc

    def _is_valid_llm_title(self, title: str, used_titles: List[str] = None) -> bool:
        """æ£€æŸ¥LLMè¿”å›çš„æ ‡é¢˜æ˜¯å¦æœ‰æ•ˆ
        
        æœ‰æ•ˆæ ‡é¢˜æ¡ä»¶ï¼š
        1. é•¿åº¦ä¸º2-5ä¸ªä¸­æ–‡å­—ç¬¦
        2. ä¸æ˜¯æŠ½è±¡è¡¨è¾¾ï¼ˆå¦‚"äº‹ä¸šå…±è¯†"ã€"è´¢å¯Œå˜åŒ–"ï¼‰
        3. ä¸æ˜¯æ–­è¯/ä¸å®Œæ•´çš„å¥å­ç‰‡æ®µ
        4. æœªè¢«ä½¿ç”¨è¿‡
        """
        if used_titles is None:
            used_titles = []
            
        if not title:
            return False
            
        # ç§»é™¤å¯èƒ½çš„å‰ç¼€ç¬¦å·
        clean_title = title.replace("âœ¨", "").replace("âš¡", "").strip()
        
        # æ£€æŸ¥é•¿åº¦ï¼ˆ2-5ä¸ªä¸­æ–‡å­—ç¬¦ï¼‰
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', clean_title)
        if len(chinese_chars) < 2 or len(chinese_chars) > 5:
            return False
        
        # æ–­è¯æ£€æµ‹ - ä»¥ä¸‹ç»“å°¾çš„æ ‡é¢˜æ˜¯ä¸å®Œæ•´çš„å¥å­ç‰‡æ®µ
        broken_endings = [
            "å°†", "æŠŠ", "è¢«", "è®©", "ä½¿", "ç»™", "å‘", "å¾€", "æœ",  # ä»‹è¯/åŠ©è¯
            "çš„", "åœ°", "å¾—", "ç€", "äº†", "è¿‡",  # åŠ©è¯
            "æ˜¯", "åœ¨", "æœ‰", "å’Œ", "ä¸", "æˆ–", "åŠ",  # åŠ¨è¯/è¿è¯
            "è€Œ", "ä½†", "å´", "å¹¶", "ä¸”", "ä¹Ÿ", "éƒ½",  # è¿è¯/å‰¯è¯
            "èƒ½", "ä¼š", "å¯", "è¦", "åº”", "è¯¥", "éœ€",  # èƒ½æ„¿åŠ¨è¯
            "å¾ˆ", "å¤ª", "æœ€", "æ›´", "è¾ƒ", "æ¯”",  # ç¨‹åº¦å‰¯è¯
            "è¿™", "é‚£", "å…¶", "æŸ", "æ¯", "å„",  # æŒ‡ç¤ºè¯
            "è§†", "å½“", "ä¸º", "æˆ", "åš", "å¦‚", "è‹¥",  # åŠ¨è¯/è¿è¯
            "ä»", "è‡ª", "äº", "è‡³", "åˆ°", "ä»¥", "å› ",  # ä»‹è¯
            "å¯¹", "å…³", "ç»", "é€š", "æŒ‰", "æ®"  # ä»‹è¯
        ]
        if clean_title and clean_title[-1] in broken_endings:
            return False
        
        # æ–­è¯æ£€æµ‹ - ä»¥ä¸‹å¼€å¤´çš„æ ‡é¢˜æ˜¯ä¸å®Œæ•´çš„å¥å­ç‰‡æ®µ
        broken_beginnings = [
            "çš„", "åœ°", "å¾—", "äº†", "ç€", "è¿‡",  # åŠ©è¯
            "å’Œ", "ä¸", "æˆ–", "åŠ", "å¹¶", "ä¸”",  # è¿è¯
            "è€Œ", "ä½†", "å´", "åˆ™", "ä¾¿", "å³"  # è¿è¯
        ]
        if clean_title and clean_title[0] in broken_beginnings:
            return False
        
        # æ£€æµ‹å¸¸è§çš„æ–­è¯æ¨¡å¼ï¼ˆä¸å®Œæ•´çŸ­è¯­ï¼‰
        broken_patterns = [
            r'^[å°†æŠŠè¢«è®©ä½¿ç»™å‘å¾€æœ].+[è§†å½“ä¸ºæˆåš]$',  # å¦‚"å°†å¥èº«è§†"
            r'^.+[æ˜¯åœ¨æœ‰]$',  # å¦‚"æœºä¼šæ˜¯"ã€"å‘å±•åœ¨"
            r'^[åœ¨ä»äº].+$',  # å¦‚"åœ¨äº‹ä¸š"ï¼ˆé™¤éåé¢è¿˜æœ‰å†…å®¹ï¼‰
            r'^å…³äº.+$',  # å¦‚"å…³äºè´¢å¯Œ"
            r'^å¯¹äº.+$',  # å¦‚"å¯¹äºå¥åº·"
        ]
        for pattern in broken_patterns:
            if re.match(pattern, clean_title):
                return False
        
        # æŠ½è±¡/æ— æ•ˆæ ‡é¢˜é»‘åå•
        invalid_titles = [
            "äº‹ä¸šå…±è¯†", "è´¢å¯Œå…±è¯†", "æƒ…æ„Ÿå…±è¯†", "å¥åº·å…±è¯†",
            "äº‹ä¸šå˜åŒ–", "è´¢å¯Œå˜åŒ–", "æƒ…æ„Ÿå˜åŒ–", "å¥åº·å˜åŒ–",
            "äº‹ä¸šè¿åŠ¿", "è´¢å¯Œè¿åŠ¿", "æƒ…æ„Ÿè¿åŠ¿", "å¥åº·è¿åŠ¿",
            "è¿åŠ¿èµ°å‘", "å¥åº·çŠ¶å†µ", "è´¢å¯Œåˆ†æ", "äº‹ä¸šåˆ†æ",
            "å¹´åº¦è¿åŠ¿", "æ•´ä½“è¿åŠ¿", "ç»¼åˆè¿åŠ¿", "å…±è¯†è§‚ç‚¹",
            "ç‹¬ç‰¹è§‚ç‚¹", "å‘½ç†å˜æ•°", "æ ¸å¿ƒå…±è¯†", "å¹´åº¦åˆ†æ",
            "å¹´ä»½", "äº‹ä¸š", "è´¢å¯Œ", "æƒ…æ„Ÿ", "å¥åº·", "è¿åŠ¿",
            "å…±è¯†", "å˜åŒ–", "åˆ†æ", "è§‚ç‚¹"
        ]
        if clean_title in invalid_titles:
            return False
        
        # æ£€æŸ¥æ˜¯å¦å·²ä½¿ç”¨
        if title in used_titles or clean_title in used_titles:
            return False
            
        return True

    def _extract_node_title(self, description: str, dimension: str, node_type: str, used_titles: List[str] = None) -> str:
        """ä»æè¿°ä¸­æå–2-5ä¸ªä¸­æ–‡å­—çš„æ ¸å¿ƒå…³é”®è¯ä½œä¸ºèŠ‚ç‚¹æ ‡é¢˜
                
        ç›®æ ‡ï¼šè®©ç”¨æˆ·ä¸€çœ¼çœ‹æ‡‚èŠ‚ç‚¹å†…å®¹çš„æ ¸å¿ƒä¸»é¢˜
        æ‰€æœ‰ç”Ÿæˆçš„æ ‡é¢˜éƒ½ä¼šç»è¿‡æ–­è¯æ ¡éªŒ
            
        Args:
            used_titles: å·²ä½¿ç”¨çš„æ ‡é¢˜åˆ—è¡¨ï¼Œé¿å…é‡å¤
        """
        if used_titles is None:
            used_titles = []
        
        # è¾…åŠ©å‡½æ•°ï¼šæ ¡éªŒæå–çš„æ ‡é¢˜æ˜¯å¦æœ‰æ•ˆï¼ˆéæ–­è¯ï¼‰
        def is_valid_extracted_title(title: str) -> bool:
            """æ£€æŸ¥æå–çš„æ ‡é¢˜æ˜¯å¦æ˜¯å®Œæ•´çš„è¯è¯­ï¼Œè€Œéæ–­è¯"""
            if not title:
                return False
            clean = title.replace("âœ¨", "").replace("âš¡", "").strip()
            if not clean:
                return False
            # æ–­è¯ç»“å°¾æ£€æµ‹
            broken_endings = ["å°†", "æŠŠ", "è¢«", "è®©", "ä½¿", "ç»™", "å‘", "å¾€", "æœ", "çš„", "åœ°", "å¾—", "ç€", "äº†", "è¿‡",
                              "æ˜¯", "åœ¨", "æœ‰", "å’Œ", "ä¸", "æˆ–", "åŠ", "è€Œ", "ä½†", "å´", "å¹¶", "ä¸”", "ä¹Ÿ", "éƒ½",
                              "èƒ½", "ä¼š", "å¯", "è¦", "åº”", "è¯¥", "éœ€", "å¾ˆ", "å¤ª", "æœ€", "æ›´", "è¾ƒ", "æ¯”",
                              "è¿™", "é‚£", "å…¶", "æŸ", "æ¯", "å„", "è§†", "å½“", "ä¸º", "æˆ", "åš", "å¦‚", "è‹¥",
                              "ä»", "è‡ª", "äº", "è‡³", "åˆ°", "ä»¥", "å› ", "å¯¹", "å…³", "ç»", "é€š", "æŒ‰", "æ®"]
            if clean[-1] in broken_endings:
                return False
            # æ–­è¯å¼€å¤´æ£€æµ‹
            broken_beginnings = ["çš„", "åœ°", "å¾—", "äº†", "ç€", "è¿‡", "å’Œ", "ä¸", "æˆ–", "åŠ", "å¹¶", "ä¸”", "è€Œ", "ä½†", "å´", "åˆ™", "ä¾¿", "å³"]
            if clean[0] in broken_beginnings:
                return False
            return True
                
        # æ ¸å¿ƒå…³é”®è¯åº“ - æ‰©å……æ›´å¤šå¸¸è§è¯æ±‡
        keyword_map = {
            "career": [
                ("æ™‹å‡", "æ™‹å‡"), ("å‡è¿", "å‡è¿"), ("åˆ›ä¸š", "åˆ›ä¸š"), ("è½¬å‹", "è½¬å‹"),
                ("ç¨³å®š", "ç¨³å®š"), ("çªç ´", "çªç ´"), ("è´µäºº", "è´µäººç›¸åŠ©"), ("åˆä½œ", "åˆä½œæœºä¼š"),
                ("ç«äº‰", "ç«äº‰åŠ å‰§"), ("ä¸šç»©", "ä¸šç»©æå‡"), ("å­¦ä¸š", "å­¦ä¸šè¿›æ­¥"), ("åå£°", "åå£°èµ·èµ·"),
                ("èŒä½", "èŒä½å˜åŠ¨"), ("å°äºº", "å°äººé˜²èŒƒ"), ("å‹åŠ›", "å‹åŠ›æµ‹è¯•"), ("å®˜è¿", "å®˜è¿äº¨é€š"),
                ("è€ƒè¯•", "è€ƒè¯•é¡ºåˆ©"), ("é¢è¯•", "é¢è¯•æœºä¼š"), ("èµ·ä¼", "è¿åŠ¿èµ·ä¼"), ("å˜åŠ¨", "å·¥ä½œå˜åŠ¨"),
                ("æœºé‡", "æœºé‡é™ä¸´"), ("æŒ‘æˆ˜", "æŒ‘æˆ˜æ¥ä¸´"), ("è°ƒåŠ¨", "å²—ä½è°ƒåŠ¨"), ("è¾èŒ", "ç¦»èŒé£é™©"),
                ("é¢†å¯¼", "é¢†å¯¼èµè¯†"), ("äº‹ä¸š", "äº‹ä¸šå‘å±•"), ("å·¥ä½œ", "å·¥ä½œç¯å¢ƒ"), ("åŠŸå", "åŠŸåè¿"),
                ("é¡¹ç›®", "é¡¹ç›®æ¨è¿›"), ("å®¢æˆ·", "å®¢æˆ·æ‹“å±•"), ("å›¢é˜Ÿ", "å›¢é˜Ÿåˆä½œ"), ("å†³ç­–", "é‡å¤§å†³ç­–"),
                ("èµ„æº", "èµ„æºè·å–"), ("äººè„‰", "äººè„‰æ‹“å±•"), ("èƒ½åŠ›", "èƒ½åŠ›æå‡"), ("æˆé•¿", "ä¸ªäººæˆé•¿")
            ],
            "wealth": [
                ("åè´¢", "åè´¢è¿"), ("æ­£è´¢", "æ­£è´¢ç¨³"), ("ç ´è´¢", "ç ´è´¢é£é™©"), ("æŠ•èµ„", "æŠ•èµ„æœºä¼š"),
                ("ç†è´¢", "ç†è´¢è§„åˆ’"), ("æ”¶å…¥", "æ”¶å…¥å¢é•¿"), ("è´¢è¿", "è´¢è¿èµ°å‘"), ("å®ˆè´¢", "å®ˆè´¢ä¸ºä¸Š"),
                ("æ¨ªè´¢", "æ¨ªè´¢ä¿¡å·"), ("è€—è´¢", "è€—è´¢è­¦ç¤º"), ("è´¢åº“", "è´¢åº“å……å®"), ("èµ„äº§", "èµ„äº§é…ç½®"),
                ("å€ºåŠ¡", "å€ºåŠ¡é£é™©"), ("å¼€æº", "å¼€æºèŠ‚æµ"), ("èµŒåš", "å¿ŒèµŒåš"), ("å€Ÿè´·", "å€Ÿè´·è°¨æ…"),
                ("å‘è´¢", "å‘è´¢æ—¶æœº"), ("æ”¶ç›Š", "æ”¶ç›Šå›æŠ¥"), ("äºæŸ", "äºæŸé¢„è­¦"), ("æˆ¿äº§", "æˆ¿äº§è¿"),
                ("è‚¡ç¥¨", "è‚¡å¸‚è¿"), ("åŠ è–ª", "åŠ è–ªæœºä¼š"), ("å¥–é‡‘", "å¥–é‡‘æ”¶å…¥"), ("é’±è´¢", "é’±è´¢æµåŠ¨"),
                ("ç”Ÿæ„", "ç”Ÿæ„è¿"), ("å‰¯ä¸š", "å‰¯ä¸šæ”¶å…¥"), ("è´­ç‰©", "æ¶ˆè´¹æ”¯å‡º"), ("ç»“ç®—", "è´¦åŠ¡ç»“ç®—")
            ],
            "emotion": [
                ("æ¡ƒèŠ±", "æ¡ƒèŠ±è¿"), ("å©šå§»", "å©šå§»è¿"), ("æ‹çˆ±", "æ‹çˆ±æœºä¼š"), ("æ„Ÿæƒ…", "æ„Ÿæƒ…å˜åŒ–"),
                ("å®¶åº­", "å®¶åº­å’Œç¦"), ("çŸ›ç›¾", "æ„Ÿæƒ…çŸ›ç›¾"), ("åˆ†ç¦»", "åˆ†ç¦»é£é™©"), ("å¤åˆ", "å¤åˆæœºä¼š"),
                ("è¯±æƒ‘", "å¤–ç•Œè¯±æƒ‘"), ("å­å¥³", "å­å¥³ç¼˜"), ("å­¤ç‹¬", "å­¤ç‹¬æ„Ÿ"), ("æ²Ÿé€š", "æ²Ÿé€šé—®é¢˜"),
                ("ä¿¡ä»»", "ä¿¡ä»»å±æœº"), ("ç»“å©š", "ç»“å©šæ—¶æœº"), ("ç¦»å©š", "ç¦»å©šé£é™©"), ("ç¬¬ä¸‰è€…", "ç¬¬ä¸‰è€…"),
                ("æš—æ˜§", "æš—æ˜§å…³ç³»"), ("è¡¨ç™½", "è¡¨ç™½æ—¶æœº"), ("çº¦ä¼š", "çº¦ä¼šæœºä¼š"), ("æ€€å­•", "æ€€å­•ç¼˜"),
                ("ç”Ÿè‚²", "ç”Ÿè‚²è®¡åˆ’"), ("çˆ¶æ¯", "å®¶äººå…³ç³»"), ("æœ‹å‹", "å‹æƒ…è¿"), ("ç¼˜åˆ†", "å§»ç¼˜è¿"),
                ("å¨˜å®¶", "å¨˜å®¶å…³ç³»"), ("çº·äº‰", "å…³ç³»çº·äº‰"), ("å†·æ·¡", "æ„Ÿæƒ…å†·æ·¡"), ("å‡æ¸©", "æ„Ÿæƒ…å‡æ¸©")
            ],
            "health": [
                ("å¥åº·", "å¥åº·çŠ¶æ€"), ("ç–¾ç—…", "ç–¾ç—…é¢„è­¦"), ("è°ƒå…»", "è°ƒå…»èº«ä½“"), ("å¿ƒç†", "å¿ƒç†å¥åº·"),
                ("ä¼‘æ¯", "ä¼‘æ¯è°ƒæ•´"), ("è¿åŠ¨", "è¿åŠ¨å¥èº«"), ("é¥®é£Ÿ", "é¥®é£Ÿè°ƒç†"), ("ç²¾ç¥", "ç²¾ç¥çŠ¶æ€"),
                ("ç–²åŠ³", "è¿‡åº¦ç–²åŠ³"), ("æ„å¤–", "æ„å¤–é˜²èŒƒ"), ("å¹³å®‰", "å¹³å®‰é¡ºé‚"), ("å‹åŠ›", "å‹åŠ›ç®¡ç†"),
                ("å…ç–«", "å…ç–«åŠ›"), ("ä½“è´¨", "ä½“è´¨è°ƒç†"), ("åº·å¤", "åº·å¤æœŸ"), ("è‚ èƒƒ", "è‚ èƒƒä¿å¥"),
                ("å¤±çœ ", "å¤±çœ é—®é¢˜"), ("ç„¦è™‘", "ç„¦è™‘æƒ…ç»ª"), ("æ‰‹æœ¯", "æ‰‹æœ¯é£é™©"), ("ä½é™¢", "ä½é™¢å¯èƒ½"),
                ("è¡€å…‰", "è¡€å…‰ä¹‹ç¾"), ("è½¦ç¥¸", "è½¦ç¥¸é˜²èŒƒ"), ("è·Œä¼¤", "è·Œä¼¤é£é™©"), ("å¤´ç—›", "å¤´ç—›å›°æ‰°"),
                ("è…¹éƒ¨", "è…¹éƒ¨ä¸é€‚"), ("ç–¹ç–¼", "ç–¹ç–¼é—®é¢˜"), ("ä¼ æŸ“", "ä¼ æŸ“é˜²æŠ¤"), ("æ…¢æ€§ç—…", "æ…¢æ€§ç—…")
            ]
        }
                
        # æ ¹æ®èŠ‚ç‚¹ç±»å‹æ·»åŠ å‰ç¼€
        type_prefix = {"consensus": "", "unique": "âœ¨", "variable": "âš¡"}
        prefix = type_prefix.get(node_type, "")
                
        # ä»æè¿°ä¸­åŒ¹é…å…³é”®è¯
        dim_keywords = keyword_map.get(dimension, [])
        matched_titles = []
        for keyword, title in dim_keywords:
            if keyword in description:
                full_title = f"{prefix}{title}" if prefix else title
                # æ£€æŸ¥æ˜¯å¦å·²ä½¿ç”¨ä¸”æ˜¯æœ‰æ•ˆæ ‡é¢˜
                if full_title not in used_titles and is_valid_extracted_title(full_title):
                    return full_title
                matched_titles.append(full_title)
            
        # å¦‚æœæ‰€æœ‰åŒ¹é…çš„æ ‡é¢˜éƒ½å·²ä½¿ç”¨ï¼Œå°è¯•åŠ åºå·åŒºåˆ†
        if matched_titles:
            base_title = matched_titles[0].replace(prefix, "")
            for i in range(2, 10):
                new_title = f"{prefix}{base_title}{i}" if prefix else f"{base_title}{i}"
                if new_title not in used_titles and is_valid_extracted_title(new_title):
                    return new_title
                
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œä»æè¿°ä¸­æå–æœ‰æ„ä¹‰çš„ä¸­æ–‡è¯
        clean_desc = description.replace("ã€å…±è¯†ã€‘", "").replace("ã€", "").split("ã€‘")[-1].strip()
            
        # æ‰©å±•è·³è¿‡è¯åˆ—è¡¨
        skip_words = ["åœ¨æ­¤", "å…³äº", "å¯¹äº", "å¤šæ•°", "å¤§å¸ˆ", "è®¤ä¸º", "é¢„æµ‹", "æ˜¾ç¤º", "æ ¹æ®", "è¡¨æ˜", 
                      "å¯èƒ½", "å°†ä¼š", "å»ºè®®", "éœ€è¦", "åº”è¯¥", "ä¸€å®š", "å¿…é¡»", "æ³¨æ„", "è¿™ä¸ª", "é‚£ä¸ª",
                      "å…¶ä¸­", "å› æ­¤", "æ‰€ä»¥", "ä½†æ˜¯", "å¦‚æœ", "è™½ç„¶", "ä¸è¿‡", "ç„¶è€Œ", "è€Œä¸”", "å¹¶ä¸”"]
        for sw in skip_words:
            if clean_desc.startswith(sw):
                clean_desc = clean_desc[len(sw):]
            
        # å°è¯•æå–æ›´æœ‰æ„ä¹‰çš„è¯ç»„
        meaningful_patterns = [
            r'([\u4e00-\u9fff]{2,4})è¿åŠ¿', r'([\u4e00-\u9fff]{2,4})æ–¹é¢',
            r'([\u4e00-\u9fff]{2,4})é—®é¢˜', r'([\u4e00-\u9fff]{2,4})æœºä¼š',
            r'([\u4e00-\u9fff]{2,4})é£é™©', r'([\u4e00-\u9fff]{2,4})å˜åŒ–',
            r'å…³äº([\u4e00-\u9fff]{2,4})', r'éœ€è¦([\u4e00-\u9fff]{2,4})',
            r'æ³¨æ„([\u4e00-\u9fff]{2,4})', r'æŠŠæ¡([\u4e00-\u9fff]{2,4})',
            r'([\u4e00-\u9fff]{2,4})ä¸Šå‡', r'([\u4e00-\u9fff]{2,4})ä¸‹é™',
            r'([\u4e00-\u9fff]{2,4})æå‡', r'([\u4e00-\u9fff]{2,4})è°ƒæ•´',
            r'([\u4e00-\u9fff]{2,4})ä¿æŒ', r'([\u4e00-\u9fff]{2,4})å¢é•¿',
            r'([\u4e00-\u9fff]{2,4})ç¨³å®š', r'([\u4e00-\u9fff]{2,4})æ³¢åŠ¨',
            r'([\u4e00-\u9fff]{2,4})æ—¶æœº', r'([\u4e00-\u9fff]{2,4})è½¬æŠ˜',
            r'([\u4e00-\u9fff]{2,4})çªç ´', r'([\u4e00-\u9fff]{2,4})æŒ‘æˆ˜',
            r'å¯èƒ½ä¼š([\u4e00-\u9fff]{2,4})', r'å»ºè®®([\u4e00-\u9fff]{2,4})',
            r'([\u4e00-\u9fff]{2,3})å¹´', r'ä¸‹åŠå¹´([\u4e00-\u9fff]{2,4})',
            r'ä¸ŠåŠå¹´([\u4e00-\u9fff]{2,4})'
        ]
        for pattern in meaningful_patterns:
            match = re.search(pattern, clean_desc)
            if match:
                extracted = match.group(1)
                # è·³è¿‡å¤ªç¬¼ç»Ÿçš„è¯
                generic_words = ["è¿åŠ¿", "æ–¹é¢", "æƒ…å†µ", "çŠ¶æ€", "æ—¶æœŸ", "é˜¶æ®µ", "å˜åŒ–", "å‘å±•"]
                if extracted not in generic_words and len(extracted) >= 2:
                    full_title = f"{prefix}{extracted}" if prefix else extracted
                    # æ·»åŠ æ–­è¯æ ¡éªŒ
                    if full_title not in used_titles and is_valid_extracted_title(full_title):
                        return full_title
                
        # ä¸å†ç›´æ¥å–å‰å››ä¸ªå­—ï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨ fallback æ ‡é¢˜
                
        # æœ€åå…†åº• - ä½¿ç”¨å…·ä½“çš„å»ºè®®æ€§æ ‡é¢˜ï¼Œè€Œä¸æ˜¯æŠ½è±¡çš„ç±»å‹åç§°
        fallback_titles = {
            "career": {
                "consensus": ["äº‹ä¸šç¨³ä¸­æœ‰å‡", "èŒåœºç£¨ç»ƒæœŸ", "æ—¶æœºå¾…æŠŠæ¡", "èƒ½åŠ›ç§¯ç´¯æœŸ"],
                "unique": ["è´µäººæ˜¾ç°", "è½¬å‹å¥‘æœº", "çªç ´æ–¹å‘", "åˆ›æ–°æœºä¼š"],
                "variable": ["ç«äº‰åŠ å‰§", "å˜åŠ¨é£é™©", "å†³ç­–å…³å£", "è°ƒæ•´æ—¶æœº"]
            },
            "wealth": {
                "consensus": ["è´¢è¿å¹³ç¨³", "ç¨³å¥ç†è´¢", "æ”¶å…¥æœ‰åº", "å¼€æºä¸ºä¸Š"],
                "unique": ["å¶å‘æ¨ªè´¢", "æŠ•èµ„æ—¶æœº", "å‰¯ä¸šå¯æœŸ", "åˆä½œç”Ÿè´¢"],
                "variable": ["ç ´è´¢é¢„è­¦", "è€—è´¢é˜²èŒƒ", "æŠ•èµ„è°¨æ…", "èµ„é‡‘æ³¢åŠ¨"]
            },
            "emotion": {
                "consensus": ["æ„Ÿæƒ…ç¨³å®š", "å®¶åº­å’Œç¦", "ç¼˜åˆ†å¾…å‘", "æ„Ÿæƒ…é¡ºé‚"],
                "unique": ["æ¡ƒèŠ±æ—ºç››", "å§»ç¼˜åˆ°æ¥", "å¤åˆå¯æœŸ", "æ·±åº¦è¿æ¥"],
                "variable": ["æ„Ÿæƒ…æ³¢æŠ˜", "è¯¯ä¼šé˜²èŒƒ", "ç¬¬ä¸‰è€…é˜²", "æ²Ÿé€šå…³å£"]
            },
            "health": {
                "consensus": ["èº«ä½“åº·å¥", "å¹³å®‰é¡ºé‚", "ä½“è´¨å¹³ç¨³", "è°ƒå…»ä¸ºä¸Š"],
                "unique": ["è¿åŠ¨å¥èº«", "ä½œæ¯è°ƒæ•´", "é¥®é£Ÿæ³¨æ„", "å¿ƒæ€è°ƒé€‚"],
                "variable": ["å¥åº·é¢„è­¦", "æ„å¤–é˜²èŒƒ", "æ—§ç–¾å¤å‘", "ç²¾åŠ›é€æ”¯"]
            }
        }
        
        dim_fallbacks = fallback_titles.get(dimension, fallback_titles["career"])
        type_fallbacks = dim_fallbacks.get(node_type, dim_fallbacks["consensus"])
        
        for fb_title in type_fallbacks:
            full_title = f"{prefix}{fb_title}" if prefix else fb_title
            if full_title not in used_titles and is_valid_extracted_title(full_title):
                return full_title
        
        # å¦‚æœå…¨éƒ¨ç”¨å®Œï¼ŒåŠ åºå·
        base = type_fallbacks[0]
        for i in range(2, 10):
            new_title = f"{prefix}{base}{i}" if prefix else f"{base}{i}"
            if new_title not in used_titles and is_valid_extracted_title(new_title):
                return new_title
        return f"{prefix}{base}" if prefix else base

    def _generate_fallback_graph(self, context_data: Any, future_years: int) -> Dict[str, Any]:
        """å…†åº•ç­–ç•¥ï¼šç”Ÿæˆé¥±æ»¡çš„å›¾è°±ï¼Œç¡®ä¿æ¯ä¸ªç»´åº¦éƒ½æœ‰å…±è¯†ã€å¤šä¸ªç‹¬ç‰¹è§‚ç‚¹å’Œå¤šä¸ªå˜æ•°
        context_data: å¯ä»¥æ˜¯ List[Dict] (å¤§å¸ˆæŠ¥å‘Šåˆ—è¡¨) æˆ– str (æ±‡æ€»æ–‡æœ¬)
        """
        # ç»Ÿä¸€è½¬æ¢ä¸º List[Dict] æ ¼å¼
        reports_list = []
        if isinstance(context_data, list):
            reports_list = context_data
        elif isinstance(context_data, str):
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•æŒ‰å¤§å¸ˆåˆ†éš”ï¼Œæˆ–è€…ä½œä¸ºå•ä¸€æ¥æº
            if "--- ã€" in context_data:
                parts = context_data.split("--- ã€")
                for part in parts:
                    if "ã€‘ ---" in part:
                        name_part, content = part.split("ã€‘ ---", 1)
                        reports_list.append({"name": name_part.strip(), "content": content.strip()})
            else:
                # ç®€å•å¤„ç†ï¼šæŒ‰å¹´ä»½åˆ‡åˆ†æˆ–ç›´æ¥ä½œä¸ºå†…å®¹
                reports_list = [{"name": "ä¼—å¸ˆç²¾è¦", "content": context_data}]
        
        logger.info(f"Fallback å¯åŠ¨ï¼Œè¾“å…¥æ•°æ®ç±»å‹: {type(context_data)}, è½¬æ¢åæŠ¥å‘Šæ•°: {len(reports_list)}")
        
        # é¢„å¤„ç†æŠ¥å‘Š
        preprocessed_reports = []
        for r in reports_list:
            content = r.get('content', '')
            paras = [p.strip() for p in re.split(r'[\nã€‚ï¼ï¼Ÿ]', content) if p.strip()]
            preprocessed_reports.append({
                "name": r.get('name', 'æœªçŸ¥å¤§å¸ˆ'),
                "paragraphs": paras
            })

        current_year = datetime.datetime.now().year
        nodes = []
        edges = []
                
        # æœªæ¥Nå¹´åŒ…å«å½“å‰å¹´ï¼ˆå¦‚2026å¹´ï¼Œæœªæƒ3å¹´ä¸º2026/2027/2028ï¼‰
        target_years = [f"{current_year + i}å¹´" for i in range(future_years)]
        dims = ["career", "wealth", "emotion", "health"]
        dim_names = {"career": "äº‹ä¸š", "wealth": "è´¢å¯Œ", "emotion": "æƒ…æ„Ÿ", "health": "å¥åº·"}
        type_names = {"consensus": "æ ¸å¿ƒå…±è¯†", "unique": "ç‹¬ç‰¹è§†è§’", "variable": "å‘½ç†å˜æ•°"}
                
        # å¤§å¸ˆå€™é€‰åˆ—è¡¨
        master_pool = ["å¢¨ç„", "äº‘æ¾å±…å£«", "è‰è‰ä¸", "éšé¹¤", "äº†å°˜", "éšé£", "é“å£", 
                       "çˆ¿ä½", "å¾·åš", "åšé›…", "é˜¿æ ¼é‡Œå¸•", "è‰¾è–›", "æ¯•è¾¾å“¥", "å¥¥ä¸",
                       "ç»‡å‘½è€…å¡æ´›æ–¯", "é•§å°„", "åº“åº“å°”å", "ç»å°”", "è™šç©º", "è¿¦å¶"]
            
        # å…¨å±€å»é‡è·Ÿè¸ª
        global_used_titles = []  # è·Ÿè¸ªå·²ä½¿ç”¨çš„æ ‡é¢˜
        global_used_descriptions = []  # è·Ÿè¸ªå·²ä½¿ç”¨çš„æè¿°å‰50å­—
                
        for ty in target_years:
            for dim in dims:
                used_masters = []
                        
                # ä»æŠ¥å‘Šä¸­æå–è¯¥å¹´ä»½è¯¥ç»´åº¦çš„æ‰€æœ‰ç›¸å…³å†…å®¹
                all_descriptions = self._extract_multiple_descriptions(preprocessed_reports, dim, ty, 10)
                desc_index = 0
                        
                def get_next_unique_description():
                    """ è·å–ä¸‹ä¸€ä¸ªæœªä½¿ç”¨è¿‡çš„æè¿° """
                    nonlocal desc_index
                    while desc_index < len(all_descriptions):
                        desc, master = all_descriptions[desc_index]
                        desc_index += 1
                        # æ£€æŸ¥æè¿°æ˜¯å¦å·²ä½¿ç”¨ï¼ˆç”¨å‰50å­—ä½œä¸ºæŒ‡çº¹ï¼‰
                        desc_fingerprint = desc[:50] if len(desc) >= 50 else desc
                        if desc_fingerprint not in global_used_descriptions:
                            global_used_descriptions.append(desc_fingerprint)
                            return (desc, master)
                    return ("", "å¤§å¸ˆå…±é¸£")
                        
                # 1ä¸ªå…±è¯†èŠ‚ç‚¹ - ä½¿ç”¨æ–°çš„æ±‡æ€»æ–¹æ³•
                consensus_desc = self._synthesize_consensus_description(preprocessed_reports, dim, ty)
                    
                title = self._extract_node_title(consensus_desc, dim, "consensus", global_used_titles)
                global_used_titles.append(title)
                nodes.append({"id": f"fallback_{ty}_{dim}_consensus", "properties": {
                    "name": title, "time": ty, "description": consensus_desc,
                    "master_name": "ä¼—å¸ˆå…±è¯†", "school_source": "å¤§å¸ˆç²¾è¦", "type": "consensus",
                    "impact": random.randint(6, 9), "dimension": dim}})
                        
                # 3-4ä¸ªç‹¬ç‰¹è§†è§’èŠ‚ç‚¹
                unique_count = random.randint(3, 4)
                for ui in range(unique_count):
                    desc, m_name = get_next_unique_description()
                    if m_name == "å¤§å¸ˆå…±é¸£" or m_name in used_masters or not desc:
                        available = [m for m in master_pool if m not in used_masters]
                        m_name = random.choice(available) if available else random.choice(master_pool)
                    used_masters.append(m_name)
                            
                    if desc:
                        desc = f"ã€{m_name}è§‚ç‚¹ã€‘{desc}"
                    else:
                        desc, _ = self._extract_rich_description(preprocessed_reports, dim, "")
                        desc = f"ã€{m_name}è§‚ç‚¹ã€‘{desc}" if desc else f"åœ¨æ­¤ç»´åº¦ï¼Œ{m_name}å¤§å¸ˆæ•æ‰åˆ°äº†ä¸€ä¸ªå…³é”®çš„{type_names['unique']}ã€‚"
                        
                    title = self._extract_node_title(desc, dim, "unique", global_used_titles)
                    global_used_titles.append(title)
                    nodes.append({"id": f"fallback_{ty}_{dim}_unique_{ui}", "properties": {
                        "name": title, "time": ty, "description": desc,
                        "master_name": m_name, "school_source": "å¤§å¸ˆç²¾è¦", "type": "unique",
                        "impact": random.randint(5, 8), "dimension": dim}})
                        
                # 2-3ä¸ªå˜æ•°èŠ‚ç‚¹
                variable_count = random.randint(2, 3)
                for vi in range(variable_count):
                    desc, m_name = get_next_unique_description()
                    if m_name == "å¤§å¸ˆå…±é¸£" or m_name in used_masters or not desc:
                        available = [m for m in master_pool if m not in used_masters]
                        m_name = random.choice(available) if available else random.choice(master_pool)
                    used_masters.append(m_name)
                            
                    if desc:
                        desc = f"ã€{m_name}å˜æ•°ã€‘{desc}"
                    else:
                        desc, _ = self._extract_rich_description(preprocessed_reports, dim, "")
                        desc = f"ã€{m_name}å˜æ•°ã€‘{desc}" if desc else f"åœ¨æ­¤ç»´åº¦ï¼Œ{m_name}å¤§å¸ˆæ•æ‰åˆ°äº†ä¸€ä¸ªå…³é”®çš„{type_names['variable']}ã€‚"
                        
                    title = self._extract_node_title(desc, dim, "variable", global_used_titles)
                    global_used_titles.append(title)
                    nodes.append({"id": f"fallback_{ty}_{dim}_variable_{vi}", "properties": {
                        "name": title, "time": ty, "description": desc,
                        "master_name": m_name, "school_source": "å¤§å¸ˆç²¾è¦", "type": "variable",
                        "impact": random.randint(5, 8), "dimension": dim}})
        
        # æ„å»ºæ˜Ÿå½¢å…³è”ï¼šæ‰€æœ‰ç‹¬ç‰¹è§†è§’å’Œå˜æ•°éƒ½å›´ç»•å…±è¯†èŠ‚ç‚¹
        for ty in target_years:
            for dim in dims:
                # æ‰¾åˆ°è¯¥å¹´ä»½è¯¥ç»´åº¦çš„æ‰€æœ‰èŠ‚ç‚¹
                year_dim_nodes = [n for n in nodes if n["properties"]["time"] == ty and n["properties"]["dimension"] == dim]
                consensus_node = next((n for n in year_dim_nodes if n["properties"]["type"] == "consensus"), None)
                unique_nodes = [n for n in year_dim_nodes if n["properties"]["type"] == "unique"]
                variable_nodes = [n for n in year_dim_nodes if n["properties"]["type"] == "variable"]
                
                if consensus_node:
                    # å…±è¯† -> æ¯ä¸ªç‹¬ç‰¹è§†è§’ï¼ˆæ˜Ÿå½¢ç»“æ„ï¼‰
                    for u in unique_nodes:
                        edges.append({"source": consensus_node["id"], "target": u["id"], "label": "è§†è§’å»¶ä¼¸", "type": "complement"})
                    # å…±è¯† -> æ¯ä¸ªå˜æ•°ï¼ˆæ˜Ÿå½¢ç»“æ„ï¼‰
                    for v in variable_nodes:
                        edges.append({"source": consensus_node["id"], "target": v["id"], "label": "æ½œåœ¨å˜å±€", "type": "conflict"})
        
        # ä»èŠ‚ç‚¹ä¸­æå–å…±è¯†å’Œå†²çªåˆ—è¡¨
        consensus_list = []
        conflicts_list = []
        for n in nodes:
            props = n.get("properties", {})
            if props.get("type") == "consensus":
                consensus_list.append({"text": props.get("name", ""), "impact": props.get("impact", 7)})
            elif props.get("type") == "variable":
                conflicts_list.append({"text": props.get("name", ""), "impact": props.get("impact", 6)})
        
        return {
            "graph_data": {"nodes": nodes, "edges": edges},
            "consensus": consensus_list[:10],  # å–å‰10ä¸ª
            "conflicts": conflicts_list[:10]   # å–å‰10ä¸ª
        }

    def _sanitize_result(self, result: Dict[str, Any], future_years: int, preprocessed_reports: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ¸…æ´—å’Œè¡¥å…¨æ•°æ®ï¼Œæ”¯æŒæ¯å¹´/æ¯ç»´åº¦æœ‰å¤šä¸ªç‹¬ç‰¹è§†è§’å’Œå˜æ•°
        all_descriptions = self._extract_multiple_descriptions(preprocessed_reports, dim, year, 10)
        """
        logger.info("å¼€å§‹æ¸…æ´—å›¾è°±æ•°æ®...")
        logger.info(f"è¾“å…¥result keys: {list(result.keys())}")
        logger.info(f"preprocessed_reports é•¿åº¦: {len(preprocessed_reports)}")
        
        current_year = datetime.datetime.now().year
        graph_data = result.get("graph_data", {"nodes": [], "edges": []})
        nodes = graph_data.get("nodes", [])
        edges = graph_data.get("edges", [])
                
        # æœªæ¥Nå¹´åŒ…å«å½“å‰å¹´ï¼ˆå¦‚2026å¹´ï¼Œæœªæƒ3å¹´ä¸º2026/2027/2028ï¼‰
        target_years = [f"{current_year + i}å¹´" for i in range(future_years)]
        required_dims = ["career", "wealth", "emotion", "health"]
        dim_names = {"career": "äº‹ä¸š", "wealth": "è´¢å¯Œ", "emotion": "æƒ…æ„Ÿ", "health": "å¥åº·"}
        type_names = {"consensus": "å…±è¯†", "unique": "ç‹¬ç‰¹è§†è§’", "variable": "å˜æ•°"}
                
        # å¤§å¸ˆå€™é€‰åˆ—è¡¨
        master_pool = ["å¢¨ç„", "äº‘æ¾å±…å£«", "è‰è‰ä¸", "éšé¹¤", "äº†å°˜", "éšé£", "é“å£", 
                       "çˆ¿ä½", "å¾·åš", "åšé›…", "é˜¿æ ¼é‡Œå¸•", "è‰¾è–›", "æ¯•è¾¾å“¥", "å¥¥ä¸",
                       "ç»‡å‘½è€…å¡æ´›æ–¯", "é•§å°„", "åº“åº“å°”å", "ç»å°”", "è™šç©º", "è¿¦å¶"]
        
        valid_nodes = []
            
        # å…¨å±€å»é‡è·Ÿè¸ª
        global_used_titles = []  # è·Ÿè¸ªå·²ä½¿ç”¨çš„æ ‡é¢˜
        global_used_descriptions = []  # è·Ÿè¸ªå·²ä½¿ç”¨çš„æè¿°å‰50å­—
                
        # å»ºç«‹ç´¢å¼•ä»¥ä¾¿å¿«é€Ÿæ£€æŸ¥ - æ”¯æŒå¤šä¸ªåŒç±»å‹èŠ‚ç‚¹
        node_map = {}  # (year, dim, type) -> [nodes]
        for node in nodes:
            props = node.get("properties", {})
            y = str(props.get("time", ""))
            # ç»Ÿä¸€å¹´ä»½æ ¼å¼ï¼Œå¤„ç† "2026" vs "2026å¹´"
            y_clean = y if "å¹´" in y else f"{y}å¹´"
            d = props.get("dimension", "")
            t = props.get("type", "consensus")
            if "20" in y_clean and d in required_dims:
                key = (y_clean, d, t)
                if key not in node_map:
                    node_map[key] = []
                node_map[key].append(node)
        
        logger.info(f"å»ºç«‹ node_map æˆåŠŸï¼ŒKey æ•°é‡: {len(node_map)}")
        for k, v in node_map.items():
            logger.debug(f"Key: {k}, èŠ‚ç‚¹æ•°: {len(v)}")
        
        # éå†è¡¥é½
        for year in target_years:
            for dim in required_dims:
                used_masters = []
                        
                # ä¸ºæ­¤å¹´ä»½æ­¤ç»´åº¦æå–å¤šä¸ªæè¿°
                all_descriptions = self._extract_multiple_descriptions(preprocessed_reports, dim, year, 10)
                desc_index = 0
                        
                def get_next_unique_description():
                    """ è·å–ä¸‹ä¸€ä¸ªæœªä½¿ç”¨è¿‡çš„æè¿° """
                    nonlocal desc_index
                    while desc_index < len(all_descriptions):
                        desc, master = all_descriptions[desc_index]
                        desc_index += 1
                        # æ£€æŸ¥æè¿°æ˜¯å¦å·²ä½¿ç”¨ï¼ˆç”¨å‰50å­—ä½œä¸ºæŒ‡çº¹ï¼‰
                        desc_fingerprint = desc[:50] if len(desc) >= 50 else desc
                        if desc_fingerprint not in global_used_descriptions:
                            global_used_descriptions.append(desc_fingerprint)
                            return (desc, master)
                    return ("", "å¤§å¸ˆå…±é¸£")
                        
                # 1. å¤„ç† consensus - æ¯ä¸ªç»´åº¦æ¯å¹´åªè¦ 1 ä¸ªï¼Œä½¿ç”¨æ±‡æ€»æ–¹æ³•ç”Ÿæˆå†…å®¹
                existing_consensus = node_map.get((year, dim, "consensus"), [])
                if existing_consensus:
                    node = existing_consensus[0]
                    props = node["properties"]
                    props["master_name"] = "ä¼—å¸ˆå…±è¯†"
                    
                    # å¦‚æœæè¿°å¤ªçŸ­æˆ–ä¸å¤Ÿè¯¦ç»†ï¼Œé‡æ–°ç”Ÿæˆæ±‡æ€»æè¿°
                    if len(props.get("description", "")) < 150:
                        props["description"] = self._synthesize_consensus_description(preprocessed_reports, dim, year)
                    
                    # ä¼˜å…ˆä½¿ç”¨LLMè¿”å›çš„æ ‡é¢˜ï¼Œå¦‚æœæ— æ•ˆæ‰é‡æ–°æå–
                    llm_title = props.get("name", "")
                    if self._is_valid_llm_title(llm_title, global_used_titles):
                        title = llm_title
                    else:
                        title = self._extract_node_title(props.get("description", ""), dim, "consensus", global_used_titles)
                    global_used_titles.append(title)
                    props["name"] = title
                    valid_nodes.append(node)
                else:
                    # ç”Ÿæˆæ–°çš„å…±è¯†èŠ‚ç‚¹ï¼Œä½¿ç”¨æ±‡æ€»æ–¹æ³•
                    consensus_desc = self._synthesize_consensus_description(preprocessed_reports, dim, year)
                    title = self._extract_node_title(consensus_desc, dim, "consensus", global_used_titles)
                    global_used_titles.append(title)
                    valid_nodes.append({
                        "id": f"gen_{year}_{dim}_consensus",
                        "properties": {
                            "name": title,
                            "time": year,
                            "description": consensus_desc,
                            "master_name": "ä¼—å¸ˆå…±è¯†",
                            "school_source": "å¤§å¸ˆå…±é¸£",
                            "type": "consensus",
                            "impact": random.randint(6, 9),
                            "dimension": dim
                        }
                    })
                        
                # 2. å¤„ç† unique - æ¯ä¸ªç»´åº¦æ¯å¹´éœ€è¦ 3-4 ä¸ª
                existing_unique = node_map.get((year, dim, "unique"), [])
                target_unique_count = random.randint(3, 4)
                    
                # å…ˆå¤„ç†å·²æœ‰çš„ unique èŠ‚ç‚¹ï¼Œæ£€æŸ¥å¹¶å»é‡
                unique_added = 0
                for i, node in enumerate(existing_unique):
                    if unique_added >= target_unique_count:
                        break
                    props = node["properties"]
                    desc = props.get("description", "")
                    desc_fingerprint = desc[:50] if len(desc) >= 50 else desc
                    # æ£€æŸ¥æè¿°æ˜¯å¦å·²ä½¿ç”¨
                    if desc_fingerprint in global_used_descriptions:
                        continue  # è·³è¿‡é‡å¤çš„èŠ‚ç‚¹
                    global_used_descriptions.append(desc_fingerprint)
                        
                    if not props.get("master_name") or props["master_name"] == "ä¼—å¸ˆå…±è¯†":
                        _, m_name = get_next_unique_description()
                        if m_name == "å¤§å¸ˆå…±é¸£" or m_name in used_masters:
                            available = [m for m in master_pool if m not in used_masters]
                            m_name = random.choice(available) if available else random.choice(master_pool)
                        props["master_name"] = m_name
                    used_masters.append(props["master_name"])
                        
                    # ä¼˜å…ˆä½¿ç”¨LLMè¿”å›çš„æ ‡é¢˜ï¼Œå¦‚æœæ— æ•ˆæ‰é‡æ–°æå–
                    llm_title = props.get("name", "")
                    if self._is_valid_llm_title(llm_title, global_used_titles):
                        title = llm_title
                    else:
                        title = self._extract_node_title(desc, dim, "unique", global_used_titles)
                    global_used_titles.append(title)
                    props["name"] = title
                        
                    # å¦‚æœæè¿°å¤ªçŸ­ï¼Œè¡¥å……å†…å®¹
                    if len(desc) < 150:
                        rich_desc, _ = get_next_unique_description()
                        if rich_desc:
                            # ç§»é™¤åºŸè¯æ¨¡æ¿ï¼Œç›´æ¥æ‹¼æ¥å…·ä½“å†…å®¹
                            props["description"] = f"ã€{props['master_name']}è§‚ç‚¹ã€‘{rich_desc}"
                    valid_nodes.append(node)
                    unique_added += 1
                        
                # è¡¥å……ä¸è¶³çš„ unique èŠ‚ç‚¹
                for i in range(unique_added, target_unique_count):
                    rich_desc, rich_master = get_next_unique_description()
                    if rich_master == "å¤§å¸ˆå…±é¸£" or rich_master in used_masters:
                        available = [m for m in master_pool if m not in used_masters]
                        rich_master = random.choice(available) if available else random.choice(master_pool)
                    used_masters.append(rich_master)
                    
                    # æ„å»ºè¯¦ç»†æè¿°
                    if rich_desc:
                        # ç§»é™¤åºŸè¯æ¨¡æ¿ï¼Œç›´æ¥ä½¿ç”¨æå–çš„ä¸°å¯Œå†…å®¹
                        full_desc = f"ã€{rich_master}è§‚ç‚¹ã€‘{rich_desc}"
                    else:
                        # Fallback: å¦‚æœå®åœ¨æ²¡æœ‰å†…å®¹ï¼Œä½¿ç”¨ç¨å¾®å…·ä½“ä¸€ç‚¹çš„é€šç”¨è¯­ï¼Œä½†é¿å…å¤ªæœºæ¢°
                        full_desc = f"ã€{rich_master}è§‚ç‚¹ã€‘{rich_master}å¤§å¸ˆåœ¨{year}å¹´{dim_names[dim]}æ–¹é¢æœ‰ç‹¬åˆ°è§è§£ï¼Œæé†’æ³¨æ„ç»†èŠ‚å˜åŒ–ï¼Œå…·ä½“å‰å‡¶éœ€ç»“åˆä¸ªäººå…«å­—ç»†æ¨ã€‚"
                        
                    title = self._extract_node_title(full_desc, dim, "unique", global_used_titles)
                    global_used_titles.append(title)
                    valid_nodes.append({
                        "id": f"gen_{year}_{dim}_unique_{i}",
                        "properties": {
                            "name": title,
                            "time": year,
                            "description": full_desc,
                            "master_name": rich_master,
                            "school_source": "å¤§å¸ˆå…±é¸£",
                            "type": "unique",
                            "impact": random.randint(5, 8),
                            "dimension": dim
                        }
                    })
                        
                # 3. å¤„ç† variable - æ¯ä¸ªç»´åº¦æ¯å¹´éœ€è¦ 2-3 ä¸ª
                existing_variable = node_map.get((year, dim, "variable"), [])
                target_variable_count = random.randint(2, 3)
                    
                # å…ˆå¤„ç†å·²æœ‰çš„ variable èŠ‚ç‚¹ï¼Œæ£€æŸ¥å¹¶å»é‡
                variable_added = 0
                for i, node in enumerate(existing_variable):
                    if variable_added >= target_variable_count:
                        break
                    props = node["properties"]
                    desc = props.get("description", "")
                    desc_fingerprint = desc[:50] if len(desc) >= 50 else desc
                    # æ£€æŸ¥æè¿°æ˜¯å¦å·²ä½¿ç”¨
                    if desc_fingerprint in global_used_descriptions:
                        continue  # è·³è¿‡é‡å¤çš„èŠ‚ç‚¹
                    global_used_descriptions.append(desc_fingerprint)
                        
                    if not props.get("master_name") or props["master_name"] == "ä¼—å¸ˆå…±è¯†":
                        _, m_name = get_next_unique_description()
                        if m_name == "å¤§å¸ˆå…±é¸£" or m_name in used_masters:
                            available = [m for m in master_pool if m not in used_masters]
                            m_name = random.choice(available) if available else random.choice(master_pool)
                        props["master_name"] = m_name
                    used_masters.append(props["master_name"])
                        
                    # ä¼˜å…ˆä½¿ç”¨LLMè¿”å›çš„æ ‡é¢˜ï¼Œå¦‚æœæ— æ•ˆæ‰é‡æ–°æå–
                    llm_title = props.get("name", "")
                    if self._is_valid_llm_title(llm_title, global_used_titles):
                        title = llm_title
                    else:
                        title = self._extract_node_title(desc, dim, "variable", global_used_titles)
                    global_used_titles.append(title)
                    props["name"] = title
                        
                    # å¦‚æœæè¿°å¤ªçŸ­ï¼Œè¡¥å……å†…å®¹
                    if len(desc) < 150:
                        rich_desc, _ = get_next_unique_description()
                        if rich_desc:
                            # ç§»é™¤åºŸè¯æ¨¡æ¿
                            props["description"] = f"ã€{props['master_name']}å˜æ•°ã€‘{rich_desc}"
                    valid_nodes.append(node)
                    variable_added += 1
                        
                # è¡¥å……ä¸è¶³çš„ variable èŠ‚ç‚¹
                for i in range(variable_added, target_variable_count):
                    rich_desc, rich_master = get_next_unique_description()
                    if rich_master == "å¤§å¸ˆå…±é¸£" or rich_master in used_masters:
                        available = [m for m in master_pool if m not in used_masters]
                        rich_master = random.choice(available) if available else random.choice(master_pool)
                    used_masters.append(rich_master)
                    
                    # æ„å»ºè¯¦ç»†æè¿°
                    if rich_desc:
                        # ç§»é™¤åºŸè¯æ¨¡æ¿
                        full_desc = f"ã€{rich_master}å˜æ•°ã€‘{rich_desc}"
                    else:
                        full_desc = f"ã€{rich_master}å˜æ•°ã€‘{rich_master}å¤§å¸ˆæŒ‡å‡º{year}å¹´{dim_names[dim]}å­˜åœ¨å…³é”®è½¬æŠ˜ï¼Œæœºé‡ä¸æŒ‘æˆ˜å¹¶å­˜ï¼Œéœ€çµæ´»åº”å˜ã€‚"
                        
                    title = self._extract_node_title(full_desc, dim, "variable", global_used_titles)
                    global_used_titles.append(title)
                    valid_nodes.append({
                        "id": f"gen_{year}_{dim}_variable_{i}",
                        "properties": {
                            "name": title,
                            "time": year,
                            "description": full_desc,
                            "master_name": rich_master,
                            "school_source": "å¤§å¸ˆå…±é¸£",
                            "type": "variable",
                            "impact": random.randint(5, 8),
                            "dimension": dim
                        }
                    })
    
        # å…³è”è¡¥å…¨é€»è¾‘ï¼šæ„å»ºæ˜Ÿå½¢ç»“æ„ï¼Œç‹¬ç‰¹è§†è§’å’Œå˜æ•°å›´ç»•å…±è¯†èŠ‚ç‚¹
        edges = []  # é‡å»ºè¾¹ï¼Œç¡®ä¿æ˜Ÿå½¢ç»“æ„
                
        # 1. åŒå¹´åŒç»´åº¦çš„æ˜Ÿå½¢å…³è”ï¼šå…±è¯†ä¸ºä¸­å¿ƒ
        for year in target_years:
            for dim in required_dims:
                year_dim_nodes = [n for n in valid_nodes if n["properties"]["time"] == year and n["properties"]["dimension"] == dim]
                consensus_node = next((n for n in year_dim_nodes if n["properties"]["type"] == "consensus"), None)
                unique_nodes = [n for n in year_dim_nodes if n["properties"]["type"] == "unique"]
                variable_nodes = [n for n in year_dim_nodes if n["properties"]["type"] == "variable"]
                            
                if consensus_node:
                    # å…±è¯† -> æ¯ä¸ªç‹¬ç‰¹è§†è§’ï¼ˆæ˜Ÿå½¢ç»“æ„ï¼‰
                    for u in unique_nodes:
                        edges.append({"source": consensus_node["id"], "target": u["id"], "label": "è§†è§’å»¶ä¼¸", "type": "complement"})
                    # å…±è¯† -> æ¯ä¸ªå˜æ•°ï¼ˆæ˜Ÿå½¢ç»“æ„ï¼‰
                    for v in variable_nodes:
                        edges.append({"source": consensus_node["id"], "target": v["id"], "label": "æ½œåœ¨å˜å±€", "type": "conflict"})
        
        # 2. è·¨ç»´åº¦å…³è” (äº‹ä¸š -> è´¢å¯Œ)
        for year in target_years:
            career = next((n for n in valid_nodes if n["properties"]["time"] == year and n["properties"]["dimension"] == "career" and n["properties"]["type"] == "consensus"), None)
            wealth = next((n for n in valid_nodes if n["properties"]["time"] == year and n["properties"]["dimension"] == "wealth" and n["properties"]["type"] == "consensus"), None)
            if career and wealth:
                edges.append({"source": career["id"], "target": wealth["id"], "label": "äº‹ä¸šåŒ–è´¢", "type": "causal"})
    
        graph_data["nodes"] = valid_nodes
        graph_data["edges"] = edges
        result["graph_data"] = graph_data
        
        logger.info(f"æ¸…æ´—åçš„èŠ‚ç‚¹æ•°é‡: {len(valid_nodes)}")
        logger.info(f"æ¸…æ´—åçš„è¾¹æ•°é‡: {len(edges)}")
        if valid_nodes:
            logger.info(f"ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ç¤ºä¾‹: {valid_nodes[0]}")
        
        # ç¡®ä¿ consensus å’Œ conflicts å­—æ®µå­˜åœ¨
        if not result.get("consensus"):
            result["consensus"] = [
                {"text": n["properties"].get("name", ""), "impact": n["properties"].get("impact", 7)}
                for n in valid_nodes if n["properties"].get("type") == "consensus"
            ][:10]
        if not result.get("conflicts"):
            result["conflicts"] = [
                {"text": n["properties"].get("name", ""), "impact": n["properties"].get("impact", 6)}
                for n in valid_nodes if n["properties"].get("type") == "variable"
            ][:10]
        
        logger.info(f"æœ€ç»ˆresultåŒ…å« keys: {list(result.keys())}")
        logger.info("æ¸…æ´—å®Œæˆï¼Œè¿”å›result")
        
        return result

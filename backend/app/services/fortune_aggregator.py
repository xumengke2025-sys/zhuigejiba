"""
å‘½è¿æ€»ç»“å®˜æœåŠ¡
è´Ÿè´£èšåˆ 49 ä½å¤§å¸ˆçš„æ¨æ¼”ç»“æœï¼Œæå–å…±è¯†ã€å†²çªåŠå›¾è°±æ•°æ®
"""

import json
import concurrent.futures
import random
import re
import datetime
from typing import Dict, Any, List, Optional
from ..utils.llm_client import LLMClient
from ..utils.logger import get_logger

logger = get_logger('wannian.fortune_aggregator')

GRAPH_PROMPT = """
# Role: å‘½è¿æ¶æ„å¸ˆ
# Task: åŸºäº49ä½å¤§å¸ˆé¢„æµ‹ï¼Œæ„å»ºæœªæ¥ {{future_years}} å¹´çš„"èµ›åšå¤©æœºå›¾è°±"JSONã€‚

# CRITICAL RULES - å¿…é¡»ä¸¥æ ¼éµå®ˆï¼š
1. **ä¸¥ç¦ç¼–é€ **ï¼šæ‰€æœ‰å†…å®¹å¿…é¡»åŸºäºæä¾›çš„49ä½å¤§å¸ˆæ¨æ¼”æ–‡æœ¬ï¼Œç¦æ­¢è™šæ„ä»»ä½•é¢„æµ‹å†…å®¹ã€‚
2. **æ ‡é¢˜å¿…é¡»æ¦‚æ‹¬å†…å®¹**ï¼šæ¯ä¸ªèŠ‚ç‚¹çš„ `name` å¿…é¡»å‡†ç¡®æ¦‚æ‹¬ `description` çš„æ ¸å¿ƒä¸»é¢˜ã€‚
3. **å¿…é¡»æ ‡æ³¨åŸæ–‡å¼•ç”¨**ï¼šæ¯ä¸ªèŠ‚ç‚¹å¿…é¡»åŒ…å« `source_quote` å­—æ®µï¼Œå¼•ç”¨è¯¥å¤§å¸ˆåŸæ–‡100-150å­—ã€‚
4. **èŠ‚ç‚¹æ•°é‡è¦æ±‚**ï¼šæ¯å¹´è‡³å°‘20ä¸ªèŠ‚ç‚¹ï¼ˆ4ç»´åº¦ Ã— 5èŠ‚ç‚¹ï¼‰ï¼Œç¡®ä¿è¦†ç›–å®Œæ•´ã€‚

# Requirements:

## 1. èŠ‚ç‚¹æ•°é‡ä¸åˆ†å¸ƒï¼ˆæå…¶é‡è¦ï¼‰
æ¯å¹´ç”ŸæˆèŠ‚ç‚¹æ•°é‡é™åˆ¶ï¼š
- 4ä¸ªç»´åº¦ï¼ˆäº‹ä¸š/è´¢å¯Œ/æƒ…æ„Ÿ/å¥åº·ï¼‰
- æ¯ä¸ªç»´åº¦é™åˆ¶ï¼š
  * **consensus (å…±è¯†) 1ä¸ª**ï¼šå¿…é¡»èšåˆå¤šä½å¤§å¸ˆè§‚ç‚¹ã€‚
  * **unique (ç‹¬ç‰¹è§‚ç‚¹) æœ€å¤š 10 ä¸ª**ï¼šåˆå¹¶ç›¸ä¼¼è§‚ç‚¹ã€‚
  * **variable (å˜æ•°) æœ€å¤š 5 ä¸ª**ï¼šåˆå¹¶ç›¸ä¼¼å˜æ•°ã€‚

## 2. èŠ‚ç‚¹æ•°æ®ç»“æ„ï¼ˆCRITICALï¼‰
æ¯ä¸ªèŠ‚ç‚¹å¿…é¡»åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
```json
{
  "id": "n1",
  "properties": {
    "name": "2-5å­—å‘½ç†ç‰¹å¾æ ‡é¢˜",
    "time": "2026å¹´",
    "description": "200-350å­—è¯¦ç»†åˆ†æ...",
    "master_name": "ä¼—å¸ˆå…±è¯†|å…·ä½“å¤§å¸ˆå",
    "source_quote": "ã€åŸæ–‡å¼•ç”¨ã€‘100-150å­—çš„å¤§å¸ˆåŸæ–‡ï¼Œå¿…é¡»ä¸€å­—ä¸å·®åœ°ä»è¾“å…¥æ–‡æœ¬ä¸­æå–",
    "source_master": "æ¥æºå¤§å¸ˆå§“å",
    "school_source": "å‘½ç†æµæ´¾",
    "type": "consensus|unique|variable",
    "impact": 1-10,
    "dimension": "career|wealth|emotion|health"
  }
}
```

## 3. èŠ‚ç‚¹æ ‡é¢˜è¦æ±‚ï¼ˆCRITICALï¼‰
- `name` å­—æ®µå¿…é¡»æ˜¯ **2-5 ä¸ªå­—çš„å‘½ç†ç‰¹å¾æ ‡é¢˜**
- **æ ‡é¢˜å¿…é¡»å‡†ç¡®æ¦‚æ‹¬ description çš„å†…å®¹**
- å¿…é¡»æ˜¯å…·ä½“çš„å‘½ç†ç‰¹å¾è¯ï¼š"æ™‹å‡æœºé‡"ã€"è´µäººç›¸åŠ©"ã€"æ¡ƒèŠ±æ—ºç››"ã€"è„¾èƒƒè°ƒå…»"
- **ç»å¯¹ç¦æ­¢æŠ½è±¡è¡¨è¾¾**ï¼š"äº‹ä¸šå…±è¯†"ã€"è´¢å¯Œå˜åŒ–"ã€"è¿åŠ¿èµ°å‘"

## 4. èŠ‚ç‚¹æè¿°è¦æ±‚ï¼ˆCRITICALï¼‰
- **é•¿åº¦**ï¼š200-350å­—ï¼Œä¸¥ç¦çŸ­äº150å­—
- **ç»“æ„è¦æ±‚**ï¼š
  * å¼€å¤´å¿…é¡»æ ‡æ³¨ï¼šã€Œã€æ¥æºï¼šXXXå¤§å¸ˆã€‘ã€
  * ç´§æ¥ç€æ ‡æ³¨åŸæ–‡å¼•ç”¨ï¼šã€ŒåŸæ–‡ï¼š"..."ã€
  * ç„¶åæ˜¯åˆ†æè§£è¯»ï¼ˆä¿ç•™å¤§å¸ˆåŸå§‹è¯­é£å’Œä¸“ä¸šæœ¯è¯­ï¼‰
- **ä¸¥ç¦ç¼–é€ **ï¼šå¿…é¡»åŸºäºåŸæ–‡ï¼Œç¦æ­¢æ”¹å†™ä¸ºé€šç”¨åºŸè¯

## 5. åŸæ–‡å¼•ç”¨è¦æ±‚ï¼ˆCRITICAL - æå…¶é‡è¦ï¼‰
- `source_quote` å­—æ®µå¿…é¡»åŒ…å«100-150å­—çš„å¤§å¸ˆåŸæ–‡
- **å¿…é¡»ä¸€å­—ä¸å·®**åœ°ä»è¾“å…¥æ–‡æœ¬ä¸­æå–
- ç”¨äºéªŒè¯å†…å®¹çœŸå®æ€§ï¼Œç”¨æˆ·å¯ä»¥çœ‹åˆ°å¤§å¸ˆåŸè¯

## 6. æº¯æºè¦æ±‚
- `master_name`ï¼šèŠ‚ç‚¹å½’å±ï¼ˆ"ä¼—å¸ˆå…±è¯†"æˆ–å…·ä½“å¤§å¸ˆåï¼‰
- `source_master`ï¼šå…·ä½“æ¥æºå¤§å¸ˆå§“å
- `source_quote`ï¼šè¯¥å¤§å¸ˆçš„åŸæ–‡å¼•ç”¨

## 7. 49ä½å¤§å¸ˆèšåˆé€»è¾‘
- ä½ ä¼šæ”¶åˆ°49ä½å¤§å¸ˆçš„æ¨æ¼”æ–‡æœ¬ï¼Œæ¯æ®µä»¥ `--- ã€å¤§å¸ˆåã€‘ ---` å¼€å¤´
- **æŒ‰å¤§å¸ˆéå†**ï¼šé€ä½åˆ†ææ¯ä½å¤§å¸ˆçš„é¢„æµ‹
- ä¸ºæ¯ä½å¤§å¸ˆçš„æ¯ä¸ªé‡è¦è§‚ç‚¹ç”ŸæˆèŠ‚ç‚¹
- ç¡®ä¿æ¯ä½å¤§å¸ˆçš„è§‚ç‚¹éƒ½æœ‰æœºä¼šè¢«çº³å…¥ï¼ˆè‡³å°‘1-2ä¸ªèŠ‚ç‚¹ï¼‰

## 8. å…³è”æ€§
å¿…é¡»æ„å»ºèŠ‚ç‚¹é—´çš„ `edges`ï¼Œå…³ç³»ç±»å‹ï¼š
- "å› æœ" (Causal): ä¸€ä¸ªäº‹ä»¶å¯¼è‡´å¦ä¸€ä¸ª
- "å¯¹å†²" (Conflict): ä¸¤ä¸ªç»´åº¦é—´çš„çŸ›ç›¾
- "äº’è¡¥" (Complement): äº’ç›¸ä¿ƒè¿›
- "æ—¶åº" (Sequence): è·¨å¹´ä»½çš„å½±å“

# Output JSON Structure:
{
  "graph_data": {
    "nodes": [
      {
        "id": "n1",
        "properties": {
          "name": "2-5å­—å‘½ç†ç‰¹å¾æ ‡é¢˜",
          "time": "2026å¹´",
          "description": "ã€æ¥æºï¼šç´«å¾®æ–—æ•°å¤§å¸ˆã€‘åŸæ–‡ï¼š\"...\" åˆ†æè§£è¯»...",
          "master_name": "å…·ä½“å¤§å¸ˆå",
          "source_quote": "100-150å­—åŸæ–‡å¼•ç”¨",
          "source_master": "æ¥æºå¤§å¸ˆå§“å",
          "school_source": "å‘½ç†æµæ´¾",
          "type": "consensus|unique|variable",
          "impact": 1-10,
          "dimension": "career|wealth|emotion|health"
        }
      }
    ],
    "edges": [{"source": "n1", "target": "n2", "label": "å…³ç³»æè¿°", "type": "causal|conflict|complement|sequence"}]
  },
  "consensus": ["å…±è¯†ç‚¹1", "å…±è¯†ç‚¹2"],
  "conflicts": ["å†²çªç‚¹1", "å†²çªç‚¹2"]
}

# æ•°é‡æ£€æŸ¥æ¸…å•ï¼ˆç”Ÿæˆåè‡ªæ£€ï¼‰ï¼š
- [ ] æ¯å¹´èŠ‚ç‚¹æ•° â‰¥ 20ä¸ªï¼ˆåŸºç¡€ä¿éšœï¼‰
- [ ] æ¯ä¸ªç»´åº¦è‡³å°‘æœ‰1ä¸ªconsensusèŠ‚ç‚¹
- [ ] å°½å¯èƒ½å¤šåœ°åŒ…å« unique å’Œ variable èŠ‚ç‚¹
- [ ] æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰ source_quote å­—æ®µ
- [ ] æ¯ä¸ªèŠ‚ç‚¹ description éƒ½åŒ…å«æ¥æºæ ‡æ³¨å’ŒåŸæ–‡å¼•ç”¨
- [ ] è¦†ç›–æ‰€æœ‰å¹´ä»½ï¼ˆ{{future_years}}å¹´ï¼‰
"""

# åˆ†å¹´ç”Ÿæˆå›¾è°±çš„æç¤ºè¯ - ç”¨äºå‡è½»å•æ¬¡LLMè°ƒç”¨è´Ÿæ‹…
YEARLY_GRAPH_PROMPT = """
# Role: å‘½è¿æ¶æ„å¸ˆ
# Task: åŸºäº49ä½å¤§å¸ˆé¢„æµ‹ï¼Œæ„å»º **{{year}}å¹´** çš„"èµ›åšå¤©æœºå›¾è°±"JSONã€‚

# CRITICAL RULESï¼š
1. **åªå…³æ³¨ {{year}} å¹´**ï¼Œä¸è¦ç”Ÿæˆå…¶ä»–å¹´ä»½çš„å†…å®¹
2. **ä¸¥ç¦ç¼–é€ **ï¼šæ‰€æœ‰å†…å®¹å¿…é¡»åŸºäºæä¾›çš„49ä½å¤§å¸ˆæ¨æ¼”æ–‡æœ¬
3. **æ ‡é¢˜å¿…é¡»æ¦‚æ‹¬å†…å®¹**ï¼šæ¯ä¸ªèŠ‚ç‚¹çš„ `name` å¿…é¡»å‡†ç¡®æ¦‚æ‹¬ `description`
4. **å¿…é¡»æ ‡æ³¨åŸæ–‡å¼•ç”¨**ï¼šæ¯ä¸ªèŠ‚ç‚¹å¿…é¡»åŒ…å« `source_quote` å­—æ®µ

# èŠ‚ç‚¹æ•°é‡è¦æ±‚ï¼ˆ{{year}}å¹´ï¼‰ï¼š
- **ç»´åº¦çº§é™åˆ¶ï¼ˆCRITICALï¼‰**ï¼š
  * **consensus (å…±è¯†)**ï¼šæ¯ä¸ªç»´åº¦ 1 ä¸ªï¼ˆå…¨ç»´åº¦å…± 4 ä¸ªï¼‰ã€‚å¿…é¡»èšåˆ 3-5 ä½ä»¥ä¸Šå¤§å¸ˆçš„å…±åŒè§‚ç‚¹ï¼Œä¸¥ç¦åªåæ˜ ä¸€ä½å¤§å¸ˆçš„æ„è§ã€‚`master_name` ç»Ÿä¸€è®¾ä¸º "ä¼—å¸ˆå…±è¯†"ã€‚
  * **unique (ç‹¬ç‰¹è§‚ç‚¹)**ï¼š**æ¯ä¸ªç»´åº¦æœ€å¤š 10 ä¸ª**ã€‚å¦‚æœè¯†åˆ«åˆ°æ›´å¤šï¼Œä»…ä¿ç•™æœ€å…·æœ‰ä»£è¡¨æ€§ã€è·¨å¤§å¸ˆå°è¯æœ€å¤šçš„è§‚ç‚¹ã€‚
  * **variable (è½¬æŠ˜/å˜æ•°)**ï¼š**æ¯ä¸ªç»´åº¦æœ€å¤š 5 ä¸ª**ã€‚å¿…é¡»åŒ…å«å…³é”®æŠ‰æ‹©ã€æœºé‡æˆ–è½¬æŠ˜ã€‚
- **ç»´åº¦è¦†ç›–**ï¼šå¿…é¡»æ¶µç›–äº‹ä¸šã€è´¢å¯Œã€æƒ…æ„Ÿã€å¥åº· 4 ä¸ªç»´åº¦ã€‚
- **æ ¸å¿ƒè¦æ±‚ï¼šå¤šå¤§å¸ˆè§‚ç‚¹èšåˆ**ï¼š
  * **consensus (å…±è¯†)**ï¼šæ¯ä¸ªç»´åº¦ 1 ä¸ªã€‚**å¿…é¡»èšåˆ 3-5 ä½ä»¥ä¸Šå¤§å¸ˆçš„å…±åŒè§‚ç‚¹**ï¼Œä¸¥ç¦åªåæ˜ ä¸€ä½å¤§å¸ˆçš„æ„è§ã€‚`master_name` ç»Ÿä¸€è®¾ä¸º "ä¼—å¸ˆå…±è¯†"ã€‚
  * **unique (ç‹¬ç‰¹è§‚ç‚¹)**ï¼šæ¯ä¸ªç»´åº¦æœ€å¤š 10 ä¸ªã€‚å¦‚æœå¤šä½å¤§å¸ˆæœ‰ç›¸ä¼¼çš„éå…±è¯†è§‚ç‚¹ï¼Œ**å¿…é¡»åˆå¹¶ä¸ºä¸€ä¸ªèŠ‚ç‚¹**ï¼Œå¹¶åœ¨ `master_name` ä¸­åˆ—å‡ºæ‰€æœ‰è´¡çŒ®å¤§å¸ˆã€‚
  * **variable (è½¬æŠ˜/å˜æ•°)**ï¼šæ¯ä¸ªç»´åº¦æœ€å¤š 5 ä¸ªã€‚å¿…é¡»åŒ…å«æŠ‰æ‹©ã€æœºé‡æˆ–è½¬æŠ˜ã€‚å¦‚æœå¤šä¸ªå¤§å¸ˆéƒ½è¯†åˆ«åˆ°äº†åŒä¸€ä¸ªå˜æ•°ï¼Œ**å¿…é¡»åˆå¹¶**ã€‚

# èšåˆæè¿°è¦æ±‚ï¼š
- **å¤šæ¥æºæ ‡æ³¨**ï¼šåœ¨ `description` å¼€å¤´æ˜ç¡®æ ‡æ³¨ã€Œã€æ¥æºï¼šå¤§å¸ˆAã€å¤§å¸ˆBã€å¤§å¸ˆCã€‘ã€ã€‚
- **æ·±åº¦èåˆ**ï¼šå°†ä¸åŒå¤§å¸ˆæä¾›çš„ç»†èŠ‚è¿›è¡Œäº’è¡¥ï¼Œå½¢æˆä¸€æ®µé€»è¾‘è¿è´¯ã€å†…å®¹ä¸°å¯Œçš„æ·±åº¦æ¨æ¼”ã€‚

# èŠ‚ç‚¹ç±»å‹å®šä¹‰ï¼ˆCRITICALï¼‰ï¼š
1. **Consensus (å…±è¯†)**ï¼šå¤šä½å¤§å¸ˆå…±åŒæåˆ°çš„æ ¸å¿ƒè¶‹åŠ¿ã€‚
2. **Unique (ç‹¬ç‰¹è§‚ç‚¹)**ï¼šä¸å…±è¯†ä¸åŒæˆ–æ›´å…·ä½“çš„è§†è§’ï¼Œä¼˜å…ˆå±•ç¤ºå¤šå¸ˆå°è¯çš„ç‹¬ç‰¹è§è§£ã€‚
3. **Variable (è½¬æŠ˜/å˜æ•°)**ï¼šæ¶‰åŠæœºé‡ã€æŠ‰æ‹©æˆ–é£é™©çš„å…³é”®æ—¶åˆ»ã€‚ä¼˜å…ˆåˆå¹¶å¤šä½å¤§å¸ˆå…±åŒé¢„è­¦çš„å˜æ•°ã€‚

# æ¥æºå¤šæ ·æ€§è¦æ±‚ï¼ˆCRITICALï¼‰ï¼š
- **ä¸¥ç¦**åªä½¿ç”¨å°‘æ•°å‡ ä½å¤§å¸ˆï¼ˆå¦‚è‰¾è–‡ã€æ¯•è¾¾å“¥ç­‰ï¼‰çš„è§‚ç‚¹ã€‚
- å°½å¯èƒ½æŒ–æ˜ä¸åŒå¤§å¸ˆçš„è§‚ç‚¹ï¼Œç¡®ä¿æ¥æºçš„ä¸°å¯Œæ€§å’Œå¤šæ ·æ€§ã€‚

# èŠ‚ç‚¹æ•°æ®ç»“æ„ï¼š
```json
{
  "id": "{{year}}_n1",
  "properties": {
    "name": "2-5å­—å‘½ç†ç‰¹å¾æ ‡é¢˜",
    "time": "{{year}}",
    "description": "ã€æ¥æºï¼šXXXå¤§å¸ˆã€‘åŸæ–‡ï¼š\"...\" åˆ†æè§£è¯»...ï¼ˆ200-350å­—ï¼‰",
    "master_name": "ä¼—å¸ˆå…±è¯†|å…·ä½“å¤§å¸ˆå",
    "source_quote": "100-150å­—åŸæ–‡å¼•ç”¨",
    "source_master": "æ¥æºå¤§å¸ˆå§“å",
    "type": "consensus|unique|variable",
    "impact": 1-10,
    "dimension": "career|wealth|emotion|health"
  }
}
```

# æ ‡é¢˜è¦æ±‚ï¼š
- 2-5ä¸ªä¸­æ–‡å­—ç¬¦
- å…·ä½“ç‰¹å¾è¯ï¼š"æ™‹å‡æœºé‡"ã€"è´µäººç›¸åŠ©"ã€"æ¡ƒèŠ±æ—ºç››"
- **ç¦æ­¢**ï¼š"äº‹ä¸šå…±è¯†"ã€"è´¢å¯Œå˜åŒ–"ç­‰æŠ½è±¡è¡¨è¾¾

# æè¿°è¦æ±‚ï¼š
- 200-350å­—
- å¼€å¤´ï¼šã€Œã€æ¥æºï¼šXXXå¤§å¸ˆã€‘åŸæ–‡ï¼šã€Œ...ã€ã€
- ä¿ç•™å¤§å¸ˆåŸå§‹è¯­é£å’Œä¸“ä¸šæœ¯è¯­

# Output JSON Structure:
{
  "graph_data": {
    "nodes": [...],
    "edges": [{"source": "{{year}}_n1", "target": "{{year}}_n2", "label": "...", "type": "..."}]
  },
  "consensus": ["..."],
  "conflicts": ["..."]
}
"""

SUMMARY_PROMPT = """
# Role: å‘½è¿æ€»ç»“å®˜
# Task: åŸºäº49ä½å¤§å¸ˆé¢„æµ‹ï¼Œæ’°å†™ä¸€ä»½å…¨æ¡ˆè‡´è¾ Markdownã€‚
# Requirements:
1. **ä¸€è‡´æ€§**ï¼šä¸¥ç¦ç¼–é€ ã€‚ç¡®ä¿æ¯ä¸ªè§‚ç‚¹ä¸äº‹å®é€»è¾‘å»åˆã€‚
2. **ç»“æ„**ï¼šæŒ‰å¹´ä»½åŠç»´åº¦(äº‹ä¸š/è´¢å¯Œ/æƒ…æ„Ÿ/å¥åº·)ç»„ç»‡ã€‚
3. **é£æ ¼**ï¼šä¼˜ç¾è‡ªç„¶ï¼Œå°†ç»“æ„åŒ–é€»è¾‘è½¬åŒ–ä¸ºæ„Ÿæ€§è§£è¯»ã€‚
# Output Format:
## ğŸ”® æ ¸å¿ƒå…±è¯†ä¸ç‹¬ç‰¹ä¿¡å·
### ğŸŒŒ æ ¸å¿ƒå…±è¯†
...
### âš¡ ç‹¬ç‰¹ä¿¡å·
...
## ğŸ“… æœªæ¥ {{future_years}} å¹´æ—¶ç©ºæ¨æ¼”è¡¨
### 2026å¹´ (ä¸™åˆ)
#### ğŸ’¼ äº‹ä¸š
- **ä¼—å¸ˆå…±è¯†**ï¼š...
- **ç‹¬ç‰¹è§†è§’**ï¼š...
...
"""

class FortuneAggregator:
    """å‘½è¿æ€»ç»“å®˜"""
    
    def __init__(self, llm_client: Optional[LLMClient] = None):
        self.llm = llm_client or LLMClient()
    
    def aggregate_reports(
        self, 
        user_data: Dict[str, Any], 
        reports: Dict[str, Dict[str, Any]],
        on_progress: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        èšåˆæŠ¥å‘Šï¼šä½¿ç”¨åˆ†å¹´ç”Ÿæˆç­–ç•¥ï¼Œé¿å…å•æ¬¡LLMè°ƒç”¨è¶…æ—¶
        """
        if on_progress:
            on_progress(92, "æ­£åœ¨æ‹¨åŠ¨æ˜Ÿç›˜ï¼Œèƒå– 49 ä½å¤§å¸ˆæ¨æ¼”ç²¾è¦...")
            
        future_years = user_data.get("future_years", 3)
        current_year = datetime.datetime.now().year
        
        # å‡†å¤‡å¤§å¸ˆæŠ¥å‘Šæ–‡æœ¬ - ä½¿ç”¨å®Œæ•´å†…å®¹ï¼Œä¸æˆªæ–­
        reports_text_full = ""
        for agent_id, data in reports.items():
            # ä½¿ç”¨å®Œæ•´å†…å®¹ï¼Œç¡®ä¿ä¸ä¸¢å¤±ä»»ä½•ä¿¡æ¯
            reports_text_full += f"\n--- ã€{data['name']}ã€‘ ---\n{data['content']}\n"
        
        user_context = f"ç”¨æˆ·ä¿¡æ¯: {json.dumps(user_data, ensure_ascii=False)}\n\n=== 49ä½å¤§å¸ˆå®Œæ•´æ¨æ¼”æ–‡æœ¬ ===\n{reports_text_full}"
        reports_list = list(reports.values())
        
        # é¢„å¤„ç†æŠ¥å‘Šåˆ—è¡¨
        preprocessed_reports = []
        for r in reports_list:
            content = r.get('content', '')
            paras = [p.strip() for p in re.split(r'[\nã€‚ï¼ï¼Ÿ]', content) if p.strip()]
            preprocessed_reports.append({
                "name": r.get('name', 'æœªçŸ¥å¤§å¸ˆ'),
                "paragraphs": paras
            })
        
        # ä½¿ç”¨åˆ†å¹´ç”Ÿæˆç­–ç•¥ - æ¯å¹´å•ç‹¬ç”Ÿæˆï¼Œé¿å…è¶…æ—¶
        if on_progress:
            on_progress(94, "æ­£åœ¨æ ¡å‡†å¤©æ˜Ÿæ–¹ä½ï¼Œé‡‡ç”¨åˆ†å¹´å‡èšç­–ç•¥...")
        
        all_nodes = []
        all_edges = []
        all_consensus = []
        all_conflicts = []
        
        # é€å¹´ç”Ÿæˆå›¾è°±
        for i in range(future_years):
            year = current_year + i
            year_str = f"{year}å¹´"
            
            if on_progress:
                progress = 94 + (i * 5 // future_years)
                on_progress(progress, f"æ­£åœ¨å‡èš {year_str} çš„å¤©æœºå›¾è°±...")
            
            try:
                year_result = self._generate_year_graph(
                    year_str, user_context, preprocessed_reports
                )
                
                # åˆå¹¶ç»“æœ
                year_nodes = year_result.get("graph_data", {}).get("nodes", [])
                year_edges = year_result.get("graph_data", {}).get("edges", [])
                
                all_nodes.extend(year_nodes)
                all_edges.extend(year_edges)
                all_consensus.extend(year_result.get("consensus", []))
                all_conflicts.extend(year_result.get("conflicts", []))
                
                logger.info(f"{year_str} ç”Ÿæˆå®Œæˆ: {len(year_nodes)} ä¸ªèŠ‚ç‚¹")
                
            except Exception as e:
                logger.error(f"{year_str} å›¾è°±ç”Ÿæˆå¤±è´¥: {str(e)}")
                # å¦‚æœæŸä¸€å¹´å¤±è´¥ï¼Œä½¿ç”¨è¡¥å……é€»è¾‘ç”Ÿæˆè¯¥å¹´çš„èŠ‚ç‚¹
                year_nodes = self._generate_fallback_year_nodes(year_str, preprocessed_reports)
                all_nodes.extend(year_nodes)
        
        # æ„å»ºå®Œæ•´çš„å›¾è°±ç»“æœ
        graph_result = {
            "graph_data": {
                "nodes": all_nodes,
                "edges": all_edges
            },
            "consensus": all_consensus[:20],
            "conflicts": all_conflicts[:20]
        }
        
        if on_progress:
            on_progress(97, "å¤©æœºæ­£åœ¨å‡èšï¼Œæ­£åœ¨ç¼–æ’°å…¨æ¡ˆè‡´è¾...")
        
        # ç”Ÿæˆæ€»ç»“æ–‡æœ¬ï¼ˆç®€åŒ–ç‰ˆï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¹´ä»½çš„èŠ‚ç‚¹ä¿¡æ¯ï¼‰
        try:
            summary_text = self._generate_summary_text(
                future_years, all_nodes, user_context
            )
            logger.info("æ€»ç»“æ–‡æœ¬ç”ŸæˆæˆåŠŸ")
        except Exception as e:
            logger.error(f"æ€»ç»“ç”Ÿæˆå¤±è´¥: {str(e)}")
            summary_text = "ï¼ˆå¤©æœºå›¾è°±å·²ç”Ÿæˆï¼Œè¯¦ç»†æ¨æ¼”è¯·æŸ¥çœ‹ä¸‹æ–¹èŠ‚ç‚¹ä¿¡æ¯ï¼‰"
        
        if on_progress:
            on_progress(98, "æ­£åœ¨æ ¡éªŒå¤©æœºå›¾è°±å®Œæ•´æ€§...")
        
        # æ•°æ®æ¸…æ´—ä¸æ ¡éªŒ
        logger.info(f"æ¸…æ´—å‰å›¾è°±èŠ‚ç‚¹æ•°: {len(all_nodes)}")
        logger.info(f"æ¸…æ´—å‰å›¾è°±è¾¹æ•°: {len(all_edges)}")
        
        graph_result = self._sanitize_result(graph_result, future_years, preprocessed_reports)
            
        # é¢„å¤„ç†æŠ¥å‘Šåˆ—è¡¨ï¼Œæå–æ®µè½
        preprocessed_reports = []
        for r in reports_list:
            content = r.get('content', '')
            paras = [p.strip() for p in re.split(r'[\nã€‚ï¼ï¼Ÿ]', content) if p.strip()]
            preprocessed_reports.append({
                "name": r.get('name', 'æœªçŸ¥å¤§å¸ˆ'),
                "paragraphs": paras
            })

        graph_result = self._sanitize_result(graph_result, future_years, preprocessed_reports)
        
        final_result = graph_result
        final_result["summary_text"] = summary_text
        
        # è°ƒè¯•æ—¥å¿—ï¼šç¡®è®¤è¿”å›æ•°æ®ç»“æ„
        logger.info("="*60)
        logger.info("æœ€ç»ˆè¿”å›æ•°æ®ç»“æ„æ£€æŸ¥ï¼š")
        logger.info(f"final_result keys: {list(final_result.keys())}")
        logger.info(f"summary_text é•¿åº¦: {len(summary_text) if summary_text else 0}")
        logger.info(f"graph_data æ˜¯å¦å­˜åœ¨: {('graph_data' in final_result)}")
        if 'graph_data' in final_result:
            logger.info(f"graph_data keys: {list(final_result['graph_data'].keys())}")
            logger.info(f"nodes æ•°é‡: {len(final_result['graph_data'].get('nodes', []))}")
            logger.info(f"edges æ•°é‡: {len(final_result['graph_data'].get('edges', []))}")
            # è¾“å‡ºå‰3ä¸ªèŠ‚ç‚¹çš„æ‘˜è¦
            for i, node in enumerate(final_result['graph_data'].get('nodes', [])[:3]):
                logger.info(f"èŠ‚ç‚¹ {i+1}: id={node.get('id')}, name={node.get('properties', {}).get('name')}, type={node.get('properties', {}).get('type')}")
        logger.info("="*60)
        
        if on_progress:
            on_progress(100, "å¤©æœºå·²ç°ï¼Œå…¨æ¡ˆæ¨æ¼”ç¼–æ’°å®Œæˆ")
            
        return final_result

    def _generate_year_graph(
        self, 
        year: str, 
        user_context: str,
        preprocessed_reports: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå•å¹´ä»½çš„å›¾è°±
        
        ä½¿ç”¨åˆ†å¹´ç­–ç•¥ï¼Œå‡è½»å•æ¬¡LLMè°ƒç”¨è´Ÿæ‹…ï¼Œé¿å…è¶…æ—¶
        """
        logger.info(f"å¼€å§‹ç”Ÿæˆ {year} çš„å›¾è°±...")
        
        # æ„å»ºæç¤ºè¯
        prompt = YEARLY_GRAPH_PROMPT.replace("{{year}}", year)
        
        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"è¯·åŸºäºä»¥ä¸‹å¤§å¸ˆæ¨æ¼”ï¼Œç”Ÿæˆ {year} çš„å‘½ç†å›¾è°±ï¼š\n{user_context}"}
        ]
        
        # è°ƒç”¨LLMç”Ÿæˆï¼Œä½¿ç”¨è¾ƒçŸ­çš„è¶…æ—¶ï¼ˆ120ç§’ï¼‰
        try:
            result = self.llm.chat_json(messages, temperature=0.3, use_boost=True)
            logger.info(f"{year} LLMç”ŸæˆæˆåŠŸ")
            
            # ç¡®ä¿èŠ‚ç‚¹IDåŒ…å«å¹´ä»½å‰ç¼€ï¼Œé¿å…å†²çª
            nodes = result.get("graph_data", {}).get("nodes", [])
            for i, node in enumerate(nodes):
                if not node.get("id", "").startswith(year.replace("å¹´", "")):
                    node["id"] = f"{year.replace('å¹´', '')}_n{i+1}"
                # ç¡®ä¿timeå­—æ®µæ­£ç¡®
                node["properties"]["time"] = year
            
            return result
            
        except Exception as e:
            logger.error(f"{year} LLMç”Ÿæˆå¤±è´¥: {str(e)}")
            raise

    def _extract_aggregated_consensus(self, preprocessed_reports: List[Dict[str, Any]], dimension: str, year: str) -> tuple:
        """ä»å¤šä½å¤§å¸ˆæŠ¥å‘Šä¸­èšåˆå…±è¯†å†…å®¹"""
        all_candidates = self._extract_all_relevant_paragraphs(preprocessed_reports, dimension, year)
        if not all_candidates:
            return "", "ç»¼åˆæ¨æ¼”"
        master_paras = {}
        for para, master in all_candidates:
            if master not in master_paras: master_paras[master] = para
        top_masters = list(master_paras.keys())[:5]
        if not top_masters: return "", "ç»¼åˆæ¨æ¼”"
        aggregated_desc = "\n".join([f"ã€{m}ã€‘ï¼š{master_paras[m]}" for m in top_masters])
        master_names = "ã€".join(top_masters)
        return aggregated_desc, master_names

    def _group_similar_candidates(self, candidates: List[tuple]) -> List[Dict]:
        """å°†ç›¸ä¼¼çš„è§‚ç‚¹åˆå¹¶ï¼Œä½“ç°å¤šå¤§å¸ˆå°è¯"""
        groups = []
        for para, master in candidates:
            found_group = False
            for group in groups:
                if master in group['masters']: continue
                common_chars = set(para) & set(group['para'])
                if len(common_chars) > 25:
                    group['masters'].append(master)
                    if len(para) > len(group['para']): group['para'] = para
                    found_group = True
                    break
            if not found_group: groups.append({'para': para, 'masters': [master]})
        return groups

    def _generate_fallback_year_nodes(self, year: str, preprocessed_reports: List[Dict[str, Any]]) -> List[Dict]:
        """å½“LLMç”Ÿæˆå¤±è´¥æ—¶ï¼Œä½¿ç”¨è§„åˆ™ç”Ÿæˆè¯¥å¹´çš„èŠ‚ç‚¹
        
        é€»è¾‘æ›´æ–°ï¼šèšåˆå¤šå¤§å¸ˆè§‚ç‚¹ï¼Œå¹¶ä¸¥æ ¼é™åˆ¶æ¯ä¸ªç»´åº¦çš„èŠ‚ç‚¹æ•°é‡ï¼ˆUnique<=10, Variable<=5ï¼‰ã€‚
        """
        logger.warning(f"ä½¿ç”¨èšåˆé€»è¾‘fallbackç”Ÿæˆ {year} çš„èŠ‚ç‚¹")
        nodes = []
        node_id = 1
        dimensions = ["career", "wealth", "emotion", "health"]
        dim_names = {"career": "äº‹ä¸š", "wealth": "è´¢å¯Œ", "emotion": "æƒ…æ„Ÿ", "health": "å¥åº·"}
        variable_keywords = ["æœºä¼š", "é€‰æ‹©", "è½¬æŠ˜", "çªç ´", "æŒ‘æˆ˜", "è‹¥", "ä¸€æ—¦", "é™¤é", "æŠ‰æ‹©", "é£é™©", "å˜æ•°", "æœºé‡"]

        for dim in dimensions:
            # 1. å…±è¯†èŠ‚ç‚¹ï¼šæ¯ä¸ªç»´åº¦ 1 ä¸ªï¼Œèšåˆå‰ 5 ä½å¤§å¸ˆ
            consensus_desc, consensus_masters = self._extract_aggregated_consensus(preprocessed_reports, dim, year)
            if consensus_desc:
                nodes.append({
                    "id": f"{year.replace('å¹´', '')}_n{node_id}",
                    "properties": {
                        "name": f"{dim_names[dim]}ä¼—å¸ˆå…±è¯†",
                        "time": year,
                        "description": f"ã€æ¥æºï¼š{consensus_masters}ã€‘\n{consensus_desc}",
                        "master_name": "ä¼—å¸ˆå…±è¯†",
                        "source_quote": consensus_desc[:150],
                        "source_master": consensus_masters,
                        "type": "consensus",
                        "impact": 8,
                        "dimension": dim
                    }
                })
                node_id += 1

            # 2. ç‹¬ç‰¹è§‚ç‚¹ä¸å˜æ•°ï¼šæŒ‰ç»´åº¦æ”¶é›†å¹¶ç­›é€‰
            all_candidates = self._extract_all_relevant_paragraphs(preprocessed_reports, dim, year)
            grouped_candidates = self._group_similar_candidates(all_candidates)
            
            dim_unique = []
            dim_variable = []
            
            for group in grouped_candidates:
                para = group['para']
                masters = group['masters']
                is_variable = any(kw in para for kw in variable_keywords)
                
                candidate_data = {
                    "para": para,
                    "masters": masters,
                    "score": len(masters) # ä»¥èšåˆå¤§å¸ˆæ•°é‡ä½œä¸ºè¯„åˆ†æ ‡å‡†
                }
                
                if is_variable:
                    dim_variable.append(candidate_data)
                else:
                    dim_unique.append(candidate_data)

            # ç­›é€‰ç‹¬ç‰¹è§‚ç‚¹ï¼šæ¯ä¸ªç»´åº¦æœ€å¤š 10 ä¸ªï¼Œä¼˜å…ˆé€‰æ‹©èšåˆå¤§å¸ˆå¤šçš„
            dim_unique.sort(key=lambda x: x['score'], reverse=True)
            for cand in dim_unique[:10]:
                para = cand['para']
                masters = cand['masters']
                m_names = "ã€".join(masters)
                nodes.append({
                    "id": f"{year.replace('å¹´', '')}_n{node_id}",
                    "properties": {
                        "name": self._extract_node_title(para, dim, "unique"),
                        "time": year,
                        "description": f"ã€æ¥æºï¼š{m_names}ã€‘\n{para}",
                        "master_name": m_names,
                        "source_quote": para[:150],
                        "source_master": masters[0],
                        "type": "unique",
                        "impact": random.randint(5, 8),
                        "dimension": dim
                    }
                })
                node_id += 1

            # ç­›é€‰å˜æ•°ï¼šæ¯ä¸ªç»´åº¦æœ€å¤š 5 ä¸ª
            dim_variable.sort(key=lambda x: x['score'], reverse=True)
            for cand in dim_variable[:5]:
                para = cand['para']
                masters = cand['masters']
                m_names = "ã€".join(masters)
                nodes.append({
                    "id": f"{year.replace('å¹´', '')}_n{node_id}",
                    "properties": {
                        "name": self._extract_node_title(para, dim, "variable"),
                        "time": year,
                        "description": f"ã€æ¥æºï¼š{m_names}ã€‘\n{para}",
                        "master_name": m_names,
                        "source_quote": para[:150],
                        "source_master": masters[0],
                        "type": "variable",
                        "impact": random.randint(7, 9),
                        "dimension": dim
                    }
                })
                node_id += 1
                
        return nodes

    def _extract_all_relevant_paragraphs(
        self, 
        preprocessed_reports: List[Dict[str, Any]], 
        dimension: str, 
        year: str
    ) -> List[tuple]:
        """æå–æ‰€æœ‰ç›¸å…³çš„æ®µè½ï¼Œä¸è¿›è¡Œè¯„åˆ†æ’åºï¼Œåªè¿‡æ»¤æ— æ•ˆå†…å®¹"""
        keywords = {
            "career": ["äº‹ä¸š", "å·¥ä½œ", "æ™‹å‡", "èŒåœº", "åˆ›ä¸š", "åå£°", "å®˜", "å­¦ä¸š", "èŒä½", "å‡è¿", "ä¸šç»©"],
            "wealth": ["è´¢å¯Œ", "é‡‘é’±", "æŠ•èµ„", "æ”¶ç›Š", "ç ´è´¢", "è´¢è¿", "é‡‘", "åˆ©", "ç†è´¢", "èµ„äº§", "æ”¶å…¥"],
            "emotion": ["æ„Ÿæƒ…", "å©šå§»", "æ‹çˆ±", "æ¡ƒèŠ±", "ä¼´ä¾£", "å®¶åº­", "æƒ…", "ç¼˜", "çˆ±æƒ…", "é…å¶", "å§»ç¼˜"],
            "health": ["å¥åº·", "èº«ä½“", "ç–¾ç—…", "å…»ç”Ÿ", "å¹³å®‰", "ç–¾", "å®‰", "ä½“è´¨", "è°ƒå…»", "åŒ»"]
        }
        
        target_keys = keywords.get(dimension, [])
        candidates = []
        
        for report in preprocessed_reports:
            master_name = report.get('name', 'æœªçŸ¥å¤§å¸ˆ')
            paragraphs = report.get('paragraphs', [])
            
            for para in paragraphs:
                # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                if len(para) < 15: 
                    continue
                
                # å¿…é¡»åŒ…å«å¹´ä»½ï¼ˆå¦‚æœæœ‰æŒ‡å®šå¹´ä»½ï¼‰
                has_year = year[:4] in para if year else False
                # å¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªç»´åº¦å…³é”®è¯
                has_keyword = any(k in para for k in target_keys)
                
                # å¦‚æœæ²¡æœ‰å¹´ä»½ï¼Œä¹Ÿæ²¡æœ‰å…³é”®è¯ï¼Œåˆ™è§†ä¸ºæ— å…³
                if not has_year and not has_keyword:
                    continue
                
                # åªè¦ç›¸å…³å°±åŠ å…¥å€™é€‰
                candidates.append((para, master_name))
        
        return candidates

    def _generate_summary_text(self, future_years: int, all_nodes: List[Dict], user_context: str) -> str:
        """ç”Ÿæˆæ€»ç»“æ–‡æœ¬
        
        ä½¿ç”¨èŠ‚ç‚¹ä¿¡æ¯ç”Ÿæˆç®€åŒ–çš„æ€»ç»“
        """
        current_year = datetime.datetime.now().year
        
        # æŒ‰å¹´ä»½å’Œç»´åº¦ç»Ÿè®¡
        year_dim_summary = {}
        for node in all_nodes:
            props = node.get("properties", {})
            year = props.get("time", "")
            dim = props.get("dimension", "")
            node_type = props.get("type", "")
            name = props.get("name", "")
            
            if year not in year_dim_summary:
                year_dim_summary[year] = {}
            if dim not in year_dim_summary[year]:
                year_dim_summary[year][dim] = {"consensus": [], "unique": [], "variable": []}
            
            year_dim_summary[year][dim][node_type].append(name)
        
        # æ„å»ºç®€åŒ–æ€»ç»“
        summary_lines = ["## ğŸ”® æ ¸å¿ƒæ¨æ¼”æ€»ç»“\n"]
        
        for i in range(future_years):
            year = f"{current_year + i}å¹´"
            if year in year_dim_summary:
                summary_lines.append(f"\n### {year}\n")
                dim_names = {"career": "äº‹ä¸š", "wealth": "è´¢å¯Œ", "emotion": "æƒ…æ„Ÿ", "health": "å¥åº·"}
                
                for dim, dim_name in dim_names.items():
                    if dim in year_dim_summary[year]:
                        consensus = year_dim_summary[year][dim].get("consensus", [])
                        if consensus:
                            summary_lines.append(f"- **{dim_name}**ï¼š{consensus[0]}\n")
        
        summary_lines.append("\n*è¯¦ç»†æ¨æ¼”è¯·æŸ¥çœ‹ä¸‹æ–¹å¤©æœºå›¾è°±*")
        
        return "".join(summary_lines)

    def _extract_rich_description(
        self, 
        preprocessed_reports: List[Dict[str, Any]], 
        dimension: str, 
        year: str, 
        exclude_texts: List[str] = None,
        exclude_masters: List[str] = None
    ) -> tuple:
        """ä»ä¸Šä¸‹æ–‡ä¸­æŠ“å–å†…å®¹ä¸°å¯Œçš„æè¿°æ–‡æœ¬åŠå¯¹åº”çš„å¤§å¸ˆå§“å
        
        Args:
            preprocessed_reports: é¢„å¤„ç†åçš„æŠ¥å‘Šåˆ—è¡¨ [{'name': '...', 'paragraphs': [...]}, ...]
            exclude_texts: å·²ä½¿ç”¨çš„æè¿°åˆ—è¡¨ï¼Œé¿å…é‡å¤æå–ç›¸åŒå†…å®¹
            exclude_masters: å·²ä½¿ç”¨è¿‡çš„å¤§å¸ˆåˆ—è¡¨ï¼Œé¿å…é‡å¤ä½¿ç”¨åŒä¸€ä½å¤§å¸ˆ
        """
        if exclude_texts is None:
            exclude_texts = []
        if exclude_masters is None:
            exclude_masters = []
            
        keywords = {
            "career": ["äº‹ä¸š", "å·¥ä½œ", "æ™‹å‡", "èŒåœº", "åˆ›ä¸š", "åå£°", "å®˜", "å­¦ä¸š", "èŒä½", "å‡è¿", "ä¸šç»©"],
            "wealth": ["è´¢å¯Œ", "é‡‘é’±", "æŠ•èµ„", "æ”¶ç›Š", "ç ´è´¢", "è´¢è¿", "é‡‘", "åˆ©", "ç†è´¢", "èµ„äº§", "æ”¶å…¥"],
            "emotion": ["æ„Ÿæƒ…", "å©šå§»", "æ‹çˆ±", "æ¡ƒèŠ±", "ä¼´ä¾£", "å®¶åº­", "æƒ…", "ç¼˜", "çˆ±æƒ…", "é…å¶", "å§»ç¼˜"],
            "health": ["å¥åº·", "èº«ä½“", "ç–¾ç—…", "å…»ç”Ÿ", "å¹³å®‰", "ç–¾", "å®‰", "ä½“è´¨", "è°ƒå…»", "åŒ»"]
        }
        
        target_keys = keywords.get(dimension, [])
        candidates = []
        
        for report in preprocessed_reports:
            master_name = report.get('name', 'æœªçŸ¥å¤§å¸ˆ')
            
            # è·³è¿‡å·²ä½¿ç”¨è¿‡çš„å¤§å¸ˆ
            if master_name in exclude_masters:
                continue
            
            paragraphs = report.get('paragraphs', [])
            
            for para in paragraphs:
                if len(para) < 15: 
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²è¢«ä½¿ç”¨
                if any(para[:50] in used for used in exclude_texts):
                    continue
                
                # ä¸å†è®¡ç®—åˆ†æ•°ï¼Œç›´æ¥æ”¶é›†æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„å€™é€‰è€…
                # åˆ†æ•°é€»è¾‘å·²åºŸå¼ƒï¼Œæ”¹ç”¨ä¸Šå±‚é€»è¾‘åŠ¨æ€ç­›é€‰
                score = 0
                has_year = year[:4] in para if year else False
                key_count = sum(1 for k in target_keys if k in para)
                
                # ç®€å•è¿‡æ»¤ï¼šå¿…é¡»åŒ…å«å…³é”®è¯æˆ–å¹´ä»½
                if has_year or key_count > 0:
                    score = 1 # æ ‡è®°ä¸ºæœ‰æ•ˆ
                
                if score > 0:
                    candidates.append((score, para, master_name))
        
        if candidates:
            # ä¸å†æŒ‰åˆ†æ•°æ’åºï¼Œä¿æŒåŸå§‹é¡ºåºæˆ–éšæœº
            # ä½†ä¸ºäº† _extract_rich_description çš„å…¼å®¹æ€§ï¼Œè¿™é‡Œæš‚æ—¶ä¿ç•™éšæœºé€‰æ‹©é€»è¾‘
            
            # ä¼˜åŒ–ï¼šå…ˆæŒ‰å¤§å¸ˆåˆ†ç»„ï¼Œå–æ¯ä½å¤§å¸ˆçš„ä¸€æ¡ï¼ˆéšæœºï¼‰ï¼Œç¡®ä¿ Top N æ¥è‡ªä¸åŒå¤§å¸ˆ
            master_candidates = {}
            for score, para, master_name in candidates:
                if master_name not in master_candidates:
                    master_candidates[master_name] = []
                master_candidates[master_name].append((score, para, master_name))
            
            # ä»æ¯ä½å¤§å¸ˆçš„å€™é€‰åˆ—è¡¨ä¸­éšæœºé€‰ä¸€æ¡
            unique_candidates = []
            for m_name, m_list in master_candidates.items():
                unique_candidates.append(random.choice(m_list))
            
            # éšæœºæ‰“ä¹±é¡ºåº
            random.shuffle(unique_candidates)
            
            # åªè¦æœ‰å¤šä¸ªå€™é€‰è€…ï¼Œå°±å°è¯•éšæœº
            # æ­¤æ—¶ unique_candidates ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ¥è‡ªä¸åŒå¤§å¸ˆ
            if unique_candidates:
                best_candidate = unique_candidates[0]
                
            full_text = best_candidate[1]
            master_name = best_candidate[2]
            
            # å¦‚æœæ®µè½å¤ªçŸ­ï¼Œå°è¯•ä»åŒä¸€ä½å¤§å¸ˆåˆå¹¶åç»­ç›¸å…³æ®µè½
            if len(full_text) < 200:
                # æ‰¾åˆ°è¯¥å¤§å¸ˆçš„æ‰€æœ‰åŸå§‹æ®µè½
                original_paras = [c for c in candidates if c[2] == master_name]
                for other_candidate in original_paras:
                    if other_candidate[1] != full_text and other_candidate[1] not in full_text:
                        full_text += " " + other_candidate[1]
                        if len(full_text) >= 200:
                            break
            
            return full_text, master_name
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–°çš„å¤§å¸ˆï¼Œå°è¯•ä»å·²æ’é™¤çš„å¤§å¸ˆä¸­æ‰¾ï¼ˆå…œåº•ï¼‰
        if exclude_masters:
            logger.warning(f"æœªæ‰¾åˆ°æ–°çš„å¤§å¸ˆæ¥æºï¼Œå°è¯•ä»å·²æ’é™¤çš„å¤§å¸ˆä¸­æŸ¥æ‰¾: {dimension}-{year}")
            return self._extract_rich_description(preprocessed_reports, dimension, year, exclude_texts, [])
        
        return "", "å¤§å¸ˆå…±é¸£"
    
    def _extract_multiple_descriptions(self, preprocessed_reports: List[Dict[str, Any]], dimension: str, year: str, count: int) -> List[tuple]:
        """ä»æŠ¥å‘Šä¸­æå–å¤šä¸ªä¸åŒçš„æè¿°"""
        results = []
        exclude_texts = []
        
        for _ in range(count * 2):
            desc, master = self._extract_rich_description(preprocessed_reports, dimension, year, exclude_texts)
            if desc and desc not in exclude_texts:
                results.append((desc, master))
                exclude_texts.append(desc)
                if len(results) >= count:
                    break
        
        return results

    def _is_valid_llm_title(self, title: str, used_titles: List[str] = None) -> bool:
        """æ£€æŸ¥LLMè¿”å›çš„æ ‡é¢˜æ˜¯å¦æœ‰æ•ˆ
        
        æœ‰æ•ˆæ ‡é¢˜æ¡ä»¶ï¼š
        1. é•¿åº¦ä¸º2-5ä¸ªä¸­æ–‡å­—ç¬¦
        2. ä¸æ˜¯æŠ½è±¡è¡¨è¾¾ï¼ˆå¦‚"äº‹ä¸šå…±è¯†"ã€"è´¢å¯Œå˜åŒ–"ï¼‰
        3. ä¸æ˜¯æ–­è¯/ä¸å®Œæ•´çš„å¥å­ç‰‡æ®µ
        4. æœªè¢«ä½¿ç”¨è¿‡
        """
        if used_titles is None:
            used_titles = []
            
        if not title:
            return False
            
        # ç§»é™¤å¯èƒ½çš„å‰ç¼€ç¬¦å·
        clean_title = title.replace("âœ¨", "").replace("âš¡", "").strip()
        
        # æ£€æŸ¥é•¿åº¦ï¼ˆ2-5ä¸ªä¸­æ–‡å­—ç¬¦ï¼‰
        chinese_chars = re.findall(r'[\u4e00-\u9fff]', clean_title)
        if len(chinese_chars) < 2 or len(chinese_chars) > 5:
            return False
        
        # æ–­è¯æ£€æµ‹ - ä»¥ä¸‹ç»“å°¾çš„æ ‡é¢˜æ˜¯ä¸å®Œæ•´çš„å¥å­ç‰‡æ®µ
        broken_endings = [
            "å°†", "æŠŠ", "è¢«", "è®©", "ä½¿", "ç»™", "å‘", "å¾€", "æœ",  # ä»‹è¯/åŠ©è¯
            "çš„", "åœ°", "å¾—", "ç€", "äº†", "è¿‡",  # åŠ©è¯
            "æ˜¯", "åœ¨", "æœ‰", "å’Œ", "ä¸", "æˆ–", "åŠ",  # åŠ¨è¯/è¿è¯
            "è€Œ", "ä½†", "å´", "å¹¶", "ä¸”", "ä¹Ÿ", "éƒ½",  # è¿è¯/å‰¯è¯
            "èƒ½", "ä¼š", "å¯", "è¦", "åº”", "è¯¥", "éœ€",  # èƒ½æ„¿åŠ¨è¯
            "å¾ˆ", "å¤ª", "æœ€", "æ›´", "è¾ƒ", "æ¯”",  # ç¨‹åº¦å‰¯è¯
            "è¿™", "é‚£", "å…¶", "æŸ", "æ¯", "å„",  # æŒ‡ç¤ºè¯
            "è§†", "å½“", "ä¸º", "æˆ", "åš", "å¦‚", "è‹¥",  # åŠ¨è¯/è¿è¯
            "ä»", "è‡ª", "äº", "è‡³", "åˆ°", "ä»¥", "å› ",  # ä»‹è¯
            "å¯¹", "å…³", "ç»", "é€š", "æŒ‰", "æ®"  # ä»‹è¯
        ]
        if clean_title and clean_title[-1] in broken_endings:
            return False
        
        # æ–­è¯æ£€æµ‹ - ä»¥ä¸‹å¼€å¤´çš„æ ‡é¢˜æ˜¯ä¸å®Œæ•´çš„å¥å­ç‰‡æ®µ
        broken_beginnings = [
            "çš„", "åœ°", "å¾—", "äº†", "ç€", "è¿‡",  # åŠ©è¯
            "å’Œ", "ä¸", "æˆ–", "åŠ", "å¹¶", "ä¸”",  # è¿è¯
            "è€Œ", "ä½†", "å´", "åˆ™", "ä¾¿", "å³"  # è¿è¯
        ]
        if clean_title and clean_title[0] in broken_beginnings:
            return False
        
        # æ£€æµ‹å¸¸è§çš„æ–­è¯æ¨¡å¼ï¼ˆä¸å®Œæ•´çŸ­è¯­ï¼‰
        broken_patterns = [
            r'^[å°†æŠŠè¢«è®©ä½¿ç»™å‘å¾€æœ].+[è§†å½“ä¸ºæˆåš]$',  # å¦‚"å°†å¥èº«è§†"
            r'^.+[æ˜¯åœ¨æœ‰]$',  # å¦‚"æœºä¼šæ˜¯"ã€"å‘å±•åœ¨"
            r'^[åœ¨ä»äº].+$',  # å¦‚"åœ¨äº‹ä¸š"ï¼ˆé™¤éåé¢è¿˜æœ‰å†…å®¹ï¼‰
            r'^å…³äº.+$',  # å¦‚"å…³äºè´¢å¯Œ"
            r'^å¯¹äº.+$',  # å¦‚"å¯¹äºå¥åº·"
        ]
        for pattern in broken_patterns:
            if re.match(pattern, clean_title):
                return False
        
        # æŠ½è±¡/æ— æ•ˆæ ‡é¢˜é»‘åå•
        invalid_titles = [
            "äº‹ä¸šå…±è¯†", "è´¢å¯Œå…±è¯†", "æƒ…æ„Ÿå…±è¯†", "å¥åº·å…±è¯†",
            "äº‹ä¸šå˜åŒ–", "è´¢å¯Œå˜åŒ–", "æƒ…æ„Ÿå˜åŒ–", "å¥åº·å˜åŒ–",
            "äº‹ä¸šè¿åŠ¿", "è´¢å¯Œè¿åŠ¿", "æƒ…æ„Ÿè¿åŠ¿", "å¥åº·è¿åŠ¿",
            "è¿åŠ¿èµ°å‘", "å¥åº·çŠ¶å†µ", "è´¢å¯Œåˆ†æ", "äº‹ä¸šåˆ†æ",
            "å¹´åº¦è¿åŠ¿", "æ•´ä½“è¿åŠ¿", "ç»¼åˆè¿åŠ¿", "å…±è¯†è§‚ç‚¹",
            "ç‹¬ç‰¹è§‚ç‚¹", "å‘½ç†å˜æ•°", "æ ¸å¿ƒå…±è¯†", "å¹´åº¦åˆ†æ",
            "å¹´ä»½", "äº‹ä¸š", "è´¢å¯Œ", "æƒ…æ„Ÿ", "å¥åº·", "è¿åŠ¿",
            "å…±è¯†", "å˜åŒ–", "åˆ†æ", "è§‚ç‚¹"
        ]
        if clean_title in invalid_titles:
            return False
        
        # æ£€æŸ¥æ˜¯å¦å·²ä½¿ç”¨
        if title in used_titles or clean_title in used_titles:
            return False
            
        return True

    def _extract_node_title(self, description: str, dimension: str, node_type: str, used_titles: List[str] = None) -> str:
        """ä»æè¿°ä¸­æå–æˆ–ç”ŸæˆèŠ‚ç‚¹æ ‡é¢˜
        
        ä¼˜å…ˆä½¿ç”¨LLMè¿”å›çš„æ ‡é¢˜ï¼Œå¦‚æœæ— æ•ˆåˆ™ä»æè¿°ä¸­æå–å…³é”®è¯
        """
        if used_titles is None:
            used_titles = []
        
        # æ ¹æ®èŠ‚ç‚¹ç±»å‹æ·»åŠ å‰ç¼€
        type_prefix = {"consensus": "", "unique": "âœ¨", "variable": "âš¡"}
        prefix = type_prefix.get(node_type, "")
        
        # æ ¸å¿ƒå…³é”®è¯åº“ - ç”¨äºä»æè¿°ä¸­åŒ¹é…
        keyword_map = {
            "career": [
                ("æ™‹å‡", "æ™‹å‡æœºé‡"), ("å‡è¿", "å‡è¿æœºä¼š"), ("åˆ›ä¸š", "åˆ›ä¸šæ—¶æœº"), ("è½¬å‹", "è½¬å‹å¥‘æœº"),
                ("è´µäºº", "è´µäººç›¸åŠ©"), ("å°äºº", "å°äººé˜²èŒƒ"), ("åˆä½œ", "åˆä½œæœºä¼š"), ("ç«äº‰", "ç«äº‰åŠ å‰§"),
                ("çªç ´", "çªç ´æ–¹å‘"), ("ç¨³å®š", "äº‹ä¸šç¨³å®š"), ("å˜åŠ¨", "å·¥ä½œå˜åŠ¨"), ("å‹åŠ›", "å‹åŠ›ç®¡ç†"),
                ("å­¦ä¸š", "å­¦ä¸šè¿›æ­¥"), ("è€ƒè¯•", "è€ƒè¯•é¡ºåˆ©"), ("é¢è¯•", "é¢è¯•æœºä¼š"), ("é¡¹ç›®", "é¡¹ç›®æ¨è¿›"),
                ("å®¢æˆ·", "å®¢æˆ·æ‹“å±•"), ("å›¢é˜Ÿ", "å›¢é˜Ÿå»ºè®¾"), ("å†³ç­–", "é‡å¤§å†³ç­–"), ("æœºé‡", "æœºé‡é™ä¸´"),
                ("æŒ‘æˆ˜", "æŒ‘æˆ˜æ¥ä¸´"), ("è°ƒåŠ¨", "å²—ä½è°ƒåŠ¨"), ("ç¦»èŒ", "ç¦»èŒé£é™©"), ("é¢†å¯¼", "é¢†å¯¼èµè¯†"),
                ("åå£°", "åå£°æå‡"), ("èŒä½", "èŒä½å˜åŠ¨"), ("ä¸šç»©", "ä¸šç»©æå‡"), ("èƒ½åŠ›", "èƒ½åŠ›æå‡"),
                ("äººè„‰", "äººè„‰æ‹“å±•"), ("èµ„æº", "èµ„æºè·å–"), ("æˆé•¿", "ä¸ªäººæˆé•¿"), ("åˆ›æ–°", "åˆ›æ–°æœºä¼š")
            ],
            "wealth": [
                ("åè´¢", "åè´¢è¿æ—º"), ("æ­£è´¢", "æ­£è´¢ç¨³å¥"), ("ç ´è´¢", "ç ´è´¢é£é™©"), ("æŠ•èµ„", "æŠ•èµ„æœºä¼š"),
                ("ç†è´¢", "ç†è´¢è§„åˆ’"), ("æ”¶å…¥", "æ”¶å…¥å¢é•¿"), ("è´¢è¿", "è´¢è¿èµ°å‘"), ("å®ˆè´¢", "å®ˆè´¢ä¸ºä¸Š"),
                ("æ¨ªè´¢", "æ¨ªè´¢ä¿¡å·"), ("è€—è´¢", "è€—è´¢è­¦ç¤º"), ("è´¢åº“", "è´¢åº“å……å®"), ("èµ„äº§", "èµ„äº§é…ç½®"),
                ("å€ºåŠ¡", "å€ºåŠ¡é£é™©"), ("å¼€æº", "å¼€æºèŠ‚æµ"), ("èµŒåš", "å¿ŒèµŒåš"), ("å€Ÿè´·", "å€Ÿè´·è°¨æ…"),
                ("å‘è´¢", "å‘è´¢æ—¶æœº"), ("æ”¶ç›Š", "æ”¶ç›Šå›æŠ¥"), ("äºæŸ", "äºæŸé¢„è­¦"), ("æˆ¿äº§", "æˆ¿äº§æŠ•èµ„"),
                ("è‚¡ç¥¨", "è‚¡å¸‚æŠ•èµ„"), ("åŠ è–ª", "åŠ è–ªæœºä¼š"), ("å¥–é‡‘", "å¥–é‡‘æ”¶å…¥"), ("é’±è´¢", "é’±è´¢æµåŠ¨"),
                ("ç”Ÿæ„", "ç”Ÿæ„ç»è¥"), ("å‰¯ä¸š", "å‰¯ä¸šæ”¶å…¥"), ("æ¶ˆè´¹", "æ¶ˆè´¹æ”¯å‡º"), ("ç»“ç®—", "è´¦åŠ¡ç»“ç®—")
            ],
            "emotion": [
                ("æ¡ƒèŠ±", "æ¡ƒèŠ±æ—ºç››"), ("å©šå§»", "å©šå§»è¿åŠ¿"), ("æ‹çˆ±", "æ‹çˆ±æœºä¼š"), ("æ„Ÿæƒ…", "æ„Ÿæƒ…å˜åŒ–"),
                ("å®¶åº­", "å®¶åº­å’Œç¦"), ("çŸ›ç›¾", "æ„Ÿæƒ…çŸ›ç›¾"), ("åˆ†ç¦»", "åˆ†ç¦»é£é™©"), ("å¤åˆ", "å¤åˆæœºä¼š"),
                ("è¯±æƒ‘", "å¤–ç•Œè¯±æƒ‘"), ("å­å¥³", "å­å¥³ç¼˜åˆ†"), ("å­¤ç‹¬", "å­¤ç‹¬æ„Ÿå¼º"), ("æ²Ÿé€š", "æ²Ÿé€šæ”¹å–„"),
                ("ä¿¡ä»»", "ä¿¡ä»»å±æœº"), ("ç»“å©š", "ç»“å©šæ—¶æœº"), ("ç¦»å©š", "ç¦»å©šé£é™©"), ("ç¬¬ä¸‰è€…", "ç¬¬ä¸‰è€…æ’è¶³"),
                ("æš—æ˜§", "æš—æ˜§å…³ç³»"), ("è¡¨ç™½", "è¡¨ç™½æ—¶æœº"), ("çº¦ä¼š", "çº¦ä¼šæœºä¼š"), ("æ€€å­•", "æ€€å­•ç¼˜åˆ†"),
                ("ç”Ÿè‚²", "ç”Ÿè‚²è®¡åˆ’"), ("çˆ¶æ¯", "çˆ¶æ¯å…³ç³»"), ("æœ‹å‹", "å‹æƒ…è¿åŠ¿"), ("ç¼˜åˆ†", "å§»ç¼˜åˆ°æ¥"),
                ("å¨˜å®¶", "å¨˜å®¶å…³ç³»"), ("çº·äº‰", "å…³ç³»çº·äº‰"), ("å†·æ·¡", "æ„Ÿæƒ…å†·æ·¡"), ("å‡æ¸©", "æ„Ÿæƒ…å‡æ¸©")
            ],
            "health": [
                ("å¥åº·", "å¥åº·çŠ¶æ€"), ("ç–¾ç—…", "ç–¾ç—…é¢„è­¦"), ("è°ƒå…»", "èº«ä½“è°ƒå…»"), ("å¿ƒç†", "å¿ƒç†å¥åº·"),
                ("ä¼‘æ¯", "ä¼‘æ¯è°ƒæ•´"), ("è¿åŠ¨", "è¿åŠ¨å¥èº«"), ("é¥®é£Ÿ", "é¥®é£Ÿè°ƒç†"), ("ç²¾ç¥", "ç²¾ç¥çŠ¶æ€"),
                ("ç–²åŠ³", "è¿‡åº¦ç–²åŠ³"), ("æ„å¤–", "æ„å¤–é˜²èŒƒ"), ("å¹³å®‰", "å¹³å®‰é¡ºé‚"), ("å‹åŠ›", "å‹åŠ›ç®¡ç†"),
                ("å…ç–«", "å…ç–«åŠ›å¼º"), ("ä½“è´¨", "ä½“è´¨è°ƒç†"), ("åº·å¤", "åº·å¤æœŸåˆ°"), ("è‚ èƒƒ", "è‚ èƒƒä¿å¥"),
                ("å¤±çœ ", "å¤±çœ å›°æ‰°"), ("ç„¦è™‘", "ç„¦è™‘æƒ…ç»ª"), ("æ‰‹æœ¯", "æ‰‹æœ¯é£é™©"), ("ä½é™¢", "ä½é™¢å¯èƒ½"),
                ("è¡€å…‰", "è¡€å…‰ä¹‹ç¾"), ("è½¦ç¥¸", "è½¦ç¥¸é˜²èŒƒ"), ("è·Œä¼¤", "è·Œä¼¤é£é™©"), ("å¤´ç—›", "å¤´ç—›å›°æ‰°"),
                ("è…¹éƒ¨", "è…¹éƒ¨ä¸é€‚"), ("ç–¼ç—›", "ç–¼ç—›é—®é¢˜"), ("ä¼ æŸ“", "ä¼ æŸ“é˜²æŠ¤"), ("æ…¢æ€§", "æ…¢æ€§ç—…ç®¡ç†")
            ]
        }
        
        # ä»æè¿°ä¸­åŒ¹é…å…³é”®è¯
        dim_keywords = keyword_map.get(dimension, [])
        matched_titles = []
        for keyword, title in dim_keywords:
            if keyword in description:
                full_title = f"{prefix}{title}" if prefix else title
                # æ£€æŸ¥æ˜¯å¦å·²ä½¿ç”¨ä¸”æ˜¯æœ‰æ•ˆæ ‡é¢˜
                if full_title not in used_titles and self._is_valid_llm_title(full_title, used_titles):
                    return full_title
                matched_titles.append(full_title)
        
        # å¦‚æœæ‰€æœ‰åŒ¹é…çš„æ ‡é¢˜éƒ½å·²ä½¿ç”¨ï¼Œå°è¯•åŠ åºå·åŒºåˆ†
        if matched_titles:
            base_title = matched_titles[0].replace(prefix, "")
            for i in range(2, 10):
                new_title = f"{prefix}{base_title}{i}" if prefix else f"{base_title}{i}"
                if new_title not in used_titles and self._is_valid_llm_title(new_title, used_titles):
                    return new_title
        
        # å…œåº•ï¼šè¿”å›ä¸€ä¸ªé€šç”¨ä½†æœ‰æ•ˆçš„æ ‡é¢˜
        fallback_titles = {
            "career": {
                "consensus": "äº‹ä¸šç¨³å¥",
                "unique": "è´µäººæ˜¾ç°",
                "variable": "å˜åŠ¨é£é™©"
            },
            "wealth": {
                "consensus": "è´¢è¿å¹³ç¨³",
                "unique": "åè´¢æœºä¼š",
                "variable": "ç ´è´¢é˜²èŒƒ"
            },
            "emotion": {
                "consensus": "æ„Ÿæƒ…é¡ºé‚",
                "unique": "æ¡ƒèŠ±æœºé‡",
                "variable": "æ„Ÿæƒ…æ³¢æŠ˜"
            },
            "health": {
                "consensus": "èº«ä½“åº·å¥",
                "unique": "å…»ç”Ÿè°ƒç†",
                "variable": "å¥åº·é¢„è­¦"
            }
        }
        
        dim_fallbacks = fallback_titles.get(dimension, fallback_titles["career"])
        type_fallback = dim_fallbacks.get(node_type, "è¿åŠ¿åˆ†æ")
        
        full_title = f"{prefix}{type_fallback}" if prefix else type_fallback
        if full_title not in used_titles:
            return full_title
        
        # å¦‚æœå…¨éƒ¨ç”¨å®Œï¼ŒåŠ åºå·
        for i in range(2, 10):
            new_title = f"{prefix}{type_fallback}{i}" if prefix else f"{type_fallback}{i}"
            if new_title not in used_titles:
                return new_title
        
        return full_title

    def _sanitize_result(self, result: Dict[str, Any], future_years: int, preprocessed_reports: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ¸…æ´—å’Œæ ¡éªŒæ•°æ®ï¼Œç¡®ä¿èŠ‚ç‚¹æ•°é‡ã€åŸæ–‡å¼•ç”¨ã€å¹´ä»½è¦†ç›–ç­‰è¦æ±‚
        
        ä¸»è¦åŠŸèƒ½ï¼š
        1. æ ¡éªŒå¹¶ä¿®å¤èŠ‚ç‚¹æ ‡é¢˜
        2. ä¸ºèŠ‚ç‚¹é™„åŠ åŸæ–‡å¼•ç”¨
        3. éªŒè¯å¹¶è¡¥å……èŠ‚ç‚¹æ•°é‡ï¼ˆæ¯å¹´è‡³å°‘20ä¸ªï¼‰
        4. ç¡®ä¿æ¯ä¸€å¹´éƒ½æœ‰èŠ‚ç‚¹è¦†ç›–
        """
        logger.info("="*60)
        logger.info("å¼€å§‹æ¸…æ´—å’Œæ ¡éªŒå›¾è°±æ•°æ®...")
        logger.info(f"è¾“å…¥result keys: {list(result.keys())}")
        logger.info(f"preprocessed_reports é•¿åº¦: {len(preprocessed_reports)}")
        
        current_year = datetime.datetime.now().year
        graph_data = result.get("graph_data", {"nodes": [], "edges": []})
        nodes = graph_data.get("nodes", [])
        edges = graph_data.get("edges", [])
        
        logger.info(f"æ¸…æ´—å‰èŠ‚ç‚¹æ•°é‡: {len(nodes)}")
        logger.info(f"æ¸…æ´—å‰è¾¹æ•°é‡: {len(edges)}")
        
        # ç¬¬ä¸€æ­¥ï¼šåŸºç¡€æ ¡éªŒå’Œæ ‡é¢˜ä¿®å¤
        valid_nodes = []
        global_used_titles = []
        
        # è·Ÿè¸ªæ¯ä¸ªå¹´ä»½-ç»´åº¦å·²ä½¿ç”¨çš„å¤§å¸ˆ
        used_masters_by_year_dim = {}
        
        for node in nodes:
            props = node.get("properties", {})
            
            # æ ¡éªŒå¿…è¦å­—æ®µ
            if not props.get("name"):
                logger.warning(f"èŠ‚ç‚¹ç¼ºå°‘nameå­—æ®µï¼Œè·³è¿‡: {node.get('id')}")
                continue
            
            if not props.get("description"):
                logger.warning(f"èŠ‚ç‚¹ç¼ºå°‘descriptionå­—æ®µï¼Œè·³è¿‡: {node.get('id')}")
                continue
            
            year = props.get("time", "")
            dimension = props.get("dimension", "career")
            node_type = props.get("type", "consensus")
            master_name = props.get("master_name", "")
            
            # åˆå§‹åŒ–è¯¥å¹´ä»½-ç»´åº¦çš„å¤§å¸ˆè·Ÿè¸ª
            key = f"{year}-{dimension}"
            if key not in used_masters_by_year_dim:
                used_masters_by_year_dim[key] = []
            
            # æ ¡éªŒæè¿°é•¿åº¦ - è¦æ±‚è‡³å°‘200å­—
            desc_len = len(props.get("description", ""))
            if desc_len < 200:
                logger.warning(f"èŠ‚ç‚¹æè¿°å¤ªçŸ­({desc_len}å­—)ï¼Œå°è¯•è¡¥å……å®Œæ•´å†…å®¹: {props.get('name')}")
                
                # æå–æ›´å¤šå†…å®¹ï¼Œç¡®ä¿è¾¾åˆ°200å­—ä»¥ä¸Šï¼Œæ’é™¤å·²ä½¿ç”¨çš„å¤§å¸ˆ
                extra_desc, source_master = self._extract_rich_description(
                    preprocessed_reports, dimension, year, 
                    exclude_masters=used_masters_by_year_dim[key]
                )
                if extra_desc:
                    # è®°å½•ä½¿ç”¨çš„å¤§å¸ˆ
                    if source_master not in used_masters_by_year_dim[key]:
                        used_masters_by_year_dim[key].append(source_master)
                    
                    # å¦‚æœæå–çš„å†…å®¹è¿˜ä¸å¤Ÿé•¿ï¼Œå°è¯•å†æå–ä¸€æ®µï¼ˆä»å…¶ä»–å¤§å¸ˆï¼‰
                    if len(extra_desc) < 150:
                        extra_desc2, master2 = self._extract_rich_description(
                            preprocessed_reports, dimension, year, 
                            [extra_desc], used_masters_by_year_dim[key]
                        )
                        if extra_desc2:
                            extra_desc += " " + extra_desc2
                            if master2 not in used_masters_by_year_dim[key]:
                                used_masters_by_year_dim[key].append(master2)
                    
                    # æ„å»ºå®Œæ•´çš„æè¿°ï¼ŒåŒ…å«æ¥æºå’ŒåŸæ–‡å¼•ç”¨
                    source_quote = extra_desc[:800] if len(extra_desc) > 800 else extra_desc
                    
                    if node_type == "consensus":
                        # å…±è¯†èŠ‚ç‚¹ï¼šæ˜¾ç¤ºä¸ºä¼—å¸ˆå…±è¯†
                        new_description = f"ã€æ¥æºï¼šä¼—å¸ˆå…±è¯†ã€‘å‚è€ƒå¤šä½å¤§å¸ˆè§‚ç‚¹ï¼šã€Œ{source_quote}ã€\n\n"
                        new_description += f"ç»¼åˆå¤šä½å¤§å¸ˆå…±è¯†ï¼š{extra_desc}\n\n"
                        new_description += f"å¤šä½å¤§å¸ˆä¸€è‡´è®¤ä¸ºï¼Œ{year}åœ¨{dimension}æ–¹é¢éœ€è¦ç‰¹åˆ«å…³æ³¨ã€‚å»ºè®®ç»“åˆä¸ªäººå®é™…æƒ…å†µï¼Œè°¨æ…å†³ç­–ã€‚"
                        display_master = "ä¼—å¸ˆå…±è¯†"
                    elif node_type == "unique":
                        new_description = f"ã€æ¥æºï¼š{source_master}ã€‘åŸæ–‡ï¼šã€Œ{source_quote}ã€\n\n"
                        new_description += f"{source_master}ç‹¬ç‰¹è§è§£ï¼š{extra_desc}\n\n"
                        new_description += f"è¿™ä¸€è§‚ç‚¹å…·æœ‰ç‹¬ç‰¹æ€§ï¼Œå€¼å¾—é‡ç‚¹å…³æ³¨ã€‚å»ºè®®ç»“åˆå…¶ä»–å¤§å¸ˆæ„è§ç»¼åˆè€ƒè™‘ã€‚"
                        display_master = source_master
                    else:  # variable
                        new_description = f"ã€æ¥æºï¼š{source_master}ã€‘åŸæ–‡ï¼šã€Œ{source_quote}ã€\n\n"
                        new_description += f"{source_master}æé†’æ³¨æ„ï¼š{extra_desc}\n\n"
                        new_description += f"è¿™æ˜¯ä¸€ä¸ªéœ€è¦ç‰¹åˆ«å…³æ³¨çš„å˜æ•°ï¼Œå¯èƒ½å­˜åœ¨ä¸ç¡®å®šæ€§ã€‚å»ºè®®æå‰åšå¥½åº”å¯¹å‡†å¤‡ã€‚"
                        display_master = source_master
                    
                    props["description"] = new_description
                    props["source_quote"] = source_quote
                    props["source_master"] = source_master
                    props["master_name"] = display_master
                    logger.info(f"èŠ‚ç‚¹å†…å®¹å·²è¡¥å……è‡³ {len(new_description)} å­— (æ˜¾ç¤ºæ¥æº: {display_master}, å®é™…æ¥æº: {source_master})")
                else:
                    logger.warning(f"æ— æ³•ä¸ºèŠ‚ç‚¹ {props.get('name')} è¡¥å……å†…å®¹")
            else:
                # èŠ‚ç‚¹å†…å®¹å·²è¶³å¤Ÿï¼Œè®°å½•å…¶æ¥æºå¤§å¸ˆ
                source_master = props.get("source_master", master_name)
                if source_master and source_master not in used_masters_by_year_dim[key]:
                    used_masters_by_year_dim[key].append(source_master)
            
            # æ ¡éªŒæ ‡é¢˜
            llm_title = props.get("name", "")
            dimension = props.get("dimension", "career")
            node_type = props.get("type", "consensus")
            
            if not self._is_valid_llm_title(llm_title, global_used_titles):
                new_title = self._extract_node_title(
                    props.get("description", ""), 
                    dimension, 
                    node_type, 
                    global_used_titles
                )
                logger.info(f"æ ‡é¢˜æ— æ•ˆï¼Œé‡æ–°æå–: '{llm_title}' -> '{new_title}'")
                props["name"] = new_title
            
            global_used_titles.append(props["name"])
            valid_nodes.append(node)
        
        logger.info(f"åŸºç¡€æ ¡éªŒåèŠ‚ç‚¹æ•°é‡: {len(valid_nodes)}")
        
        # ç¬¬äºŒæ­¥ï¼šä¸ºèŠ‚ç‚¹é™„åŠ åŸæ–‡å¼•ç”¨
        valid_nodes = self._attach_source_quotes(valid_nodes, preprocessed_reports)
        
        # ç¬¬ä¸‰æ­¥ï¼šéªŒè¯å¹¶è¡¥å……èŠ‚ç‚¹æ•°é‡ï¼ˆæ¯å¹´è‡³å°‘20ä¸ªï¼‰
        valid_nodes = self._verify_and_supplement_nodes(valid_nodes, future_years, preprocessed_reports)
        
        # é‡å»ºè¾¹ï¼ˆåªä¿ç•™ä¸¤ç«¯èŠ‚ç‚¹éƒ½å­˜åœ¨çš„è¾¹ï¼‰
        valid_node_ids = {n["id"] for n in valid_nodes}
        valid_edges = []
        for edge in edges:
            if edge.get("source") in valid_node_ids and edge.get("target") in valid_node_ids:
                valid_edges.append(edge)
        
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„è¾¹ï¼Œå¯ä»¥åŸºäºèŠ‚ç‚¹å…³ç³»ç”Ÿæˆä¸€äº›é»˜è®¤è¾¹
        if len(valid_edges) < len(valid_nodes) * 0.5:
            logger.info("è¾¹æ•°é‡ä¸è¶³ï¼Œå°è¯•ç”Ÿæˆè¡¥å……è¾¹...")
            valid_edges = self._generate_supplement_edges(valid_nodes, valid_edges)
        
        graph_data["nodes"] = valid_nodes
        graph_data["edges"] = valid_edges
        result["graph_data"] = graph_data
        
        logger.info(f"="*60)
        logger.info(f"æ¸…æ´—å®Œæˆï¼")
        logger.info(f"æœ€ç»ˆèŠ‚ç‚¹æ•°é‡: {len(valid_nodes)}")
        logger.info(f"æœ€ç»ˆè¾¹æ•°é‡: {len(valid_edges)}")
        
        # ç»Ÿè®¡æ¯å¹´èŠ‚ç‚¹æ•°
        year_counts = {}
        for node in valid_nodes:
            year = node.get("properties", {}).get("time", "")
            year_counts[year] = year_counts.get(year, 0) + 1
        logger.info(f"æ¯å¹´èŠ‚ç‚¹åˆ†å¸ƒ: {year_counts}")
        
        # ç¡®ä¿ consensus å’Œ conflicts å­—æ®µå­˜åœ¨
        if not result.get("consensus"):
            result["consensus"] = [
                {"text": n["properties"].get("name", ""), "impact": n["properties"].get("impact", 7)}
                for n in valid_nodes if n["properties"].get("type") == "consensus"
            ][:10]
        if not result.get("conflicts"):
            result["conflicts"] = [
                {"text": n["properties"].get("name", ""), "impact": n["properties"].get("impact", 6)}
                for n in valid_nodes if n["properties"].get("type") == "variable"
            ][:10]
        
        logger.info(f"æœ€ç»ˆresultåŒ…å« keys: {list(result.keys())}")
        logger.info("="*60)
        
        return result
    
    def _generate_supplement_edges(self, nodes: List[Dict], existing_edges: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆè¡¥å……çš„è¾¹å…³ç³»
        
        å½“è¾¹æ•°é‡ä¸è¶³æ—¶ï¼ŒåŸºäºèŠ‚ç‚¹çš„æ—¶é—´å’Œç»´åº¦å…³ç³»ç”Ÿæˆè¡¥å……è¾¹
        """
        edges = existing_edges.copy()
        edge_set = {(e.get("source"), e.get("target")) for e in existing_edges}
        
        # æŒ‰å¹´ä»½å’Œç»´åº¦åˆ†ç»„
        nodes_by_year_dim = {}
        for node in nodes:
            props = node.get("properties", {})
            year = props.get("time", "")
            dim = props.get("dimension", "")
            key = f"{year}-{dim}"
            if key not in nodes_by_year_dim:
                nodes_by_year_dim[key] = []
            nodes_by_year_dim[key].append(node)
        
        # ä¸ºåŒä¸€å¹´åŒä¸€ç»´åº¦çš„èŠ‚ç‚¹ç”Ÿæˆè¾¹
        for key, group_nodes in nodes_by_year_dim.items():
            if len(group_nodes) >= 2:
                # åœ¨ç»„å†…èŠ‚ç‚¹ä¹‹é—´ç”Ÿæˆè¾¹
                for i in range(len(group_nodes)):
                    for j in range(i + 1, min(i + 3, len(group_nodes))):
                        source = group_nodes[i]["id"]
                        target = group_nodes[j]["id"]
                        
                        if (source, target) not in edge_set and (target, source) not in edge_set:
                            edges.append({
                                "source": source,
                                "target": target,
                                "label": "ç›¸äº’å…³è”",
                                "type": "complement"
                            })
                            edge_set.add((source, target))
        
        return edges

    def _attach_source_quotes(self, nodes: List[Dict], preprocessed_reports: List[Dict]) -> List[Dict]:
        """ä¸ºæ¯ä¸ªèŠ‚ç‚¹é™„åŠ åŸæ–‡å¼•ç”¨
        
        Args:
            nodes: èŠ‚ç‚¹åˆ—è¡¨
            preprocessed_reports: é¢„å¤„ç†åçš„æŠ¥å‘Šåˆ—è¡¨
            
        Returns:
            é™„åŠ äº†åŸæ–‡å¼•ç”¨çš„èŠ‚ç‚¹åˆ—è¡¨
        """
        logger.info("å¼€å§‹ä¸ºèŠ‚ç‚¹é™„åŠ åŸæ–‡å¼•ç”¨...")
        
        # æ„å»ºå¤§å¸ˆååˆ°å†…å®¹çš„æ˜ å°„
        master_content_map = {}
        for report in preprocessed_reports:
            master_name = report.get('name', 'æœªçŸ¥å¤§å¸ˆ')
            content = ' '.join(report.get('paragraphs', []))
            master_content_map[master_name] = content
        
        for node in nodes:
            props = node.get("properties", {})
            master_name = props.get("master_name", "")
            description = props.get("description", "")
            
            # å¦‚æœå·²æœ‰source_quoteï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ•ˆï¼ˆè‡³å°‘100å­—ï¼‰
            existing_quote = props.get("source_quote", "")
            if existing_quote and len(existing_quote) >= 100:
                continue
            
            # ä»descriptionä¸­æå–åŸæ–‡å¼•ç”¨æ ‡è®°
            quote_match = re.search(r'åŸæ–‡ï¼š["""ã€Œ](.+?)["""ã€]', description, re.DOTALL)
            if quote_match:
                # ä¿ç•™å®Œæ•´å¼•ç”¨ï¼Œä¸æˆªæ–­ï¼ˆè‡³å°‘200å­—ï¼‰
                full_quote = quote_match.group(1).strip()
                props["source_quote"] = full_quote[:800] if len(full_quote) > 800 else full_quote
                continue
            
            # å°è¯•ä»å¤§å¸ˆå†…å®¹ä¸­æå–åŒ¹é…çš„æ®µè½
            source_master = props.get("source_master", master_name)
            if source_master in master_content_map:
                content = master_content_map[source_master]
                # æå–å®Œæ•´æ®µè½ï¼Œä¸æˆªæ–­
                paragraphs = [p.strip() for p in re.split(r'[ã€‚ï¼ï¼Ÿ\n]', content) if len(p.strip()) >= 50]
                if paragraphs:
                    # é€‰æ‹©æœ€é•¿çš„æ®µè½ä½œä¸ºåŸæ–‡å¼•ç”¨
                    best_para = max(paragraphs, key=len)
                    # ä¿ç•™200-800å­—çš„åŸæ–‡
                    if len(best_para) > 800:
                        best_para = best_para[:800]
                    props["source_quote"] = best_para
                    # æ›´æ–°descriptionï¼Œæ·»åŠ åŸæ–‡å¼•ç”¨
                    if "åŸæ–‡ï¼š" not in description:
                        props["description"] = f"ã€æ¥æºï¼š{source_master}ã€‘åŸæ–‡ï¼šã€Œ{best_para}ã€\n\n{description}"
            
            # ç¡®ä¿source_masterå­—æ®µå­˜åœ¨
            if not props.get("source_master"):
                props["source_master"] = master_name if master_name != "ä¼—å¸ˆå…±è¯†" else "å¤šä½å¤§å¸ˆ"
        
        logger.info("åŸæ–‡å¼•ç”¨é™„åŠ å®Œæˆ")
        return nodes

    def _verify_and_supplement_nodes(
        self, 
        nodes: List[Dict], 
        future_years: int, 
        preprocessed_reports: List[Dict]
    ) -> List[Dict]:
        """éªŒè¯èŠ‚ç‚¹æ•°é‡å¹¶è¡¥å……ç¼ºå¤±çš„èŠ‚ç‚¹
        
        æ›´æ–°é€»è¾‘ï¼š
        1. æ¯å¹´è‡³å°‘20ä¸ªèŠ‚ç‚¹ï¼ˆåŸºç¡€ä¿éšœï¼‰
        2. æ¯ä¸ªç»´åº¦è‡³å°‘1ä¸ªconsensusèŠ‚ç‚¹
        3. å°½å¯èƒ½å¤šåœ°ä¿ç•™ unique å’Œ variable èŠ‚ç‚¹
        """
        logger.info("å¼€å§‹éªŒè¯å¹¶è¡¥å……èŠ‚ç‚¹æ•°é‡...")
        
        current_year = datetime.datetime.now().year
        target_years = [f"{current_year + i}å¹´" for i in range(future_years)]
        required_dims = ["career", "wealth", "emotion", "health"]
        dim_names = {
            "career": "äº‹ä¸š", 
            "wealth": "è´¢å¯Œ", 
            "emotion": "æƒ…æ„Ÿ", 
            "health": "å¥åº·"
        }
        
        # ç»Ÿè®¡ç°æœ‰èŠ‚ç‚¹åˆ†å¸ƒ
        node_count = {}
        for year in target_years:
            node_count[year] = {}
            for dim in required_dims:
                node_count[year][dim] = {"consensus": [], "unique": [], "variable": []}
        
        for node in nodes:
            props = node.get("properties", {})
            year = props.get("time", "")
            dim = props.get("dimension", "")
            node_type = props.get("type", "")
            
            if year in node_count and dim in required_dims:
                if node_type not in node_count[year][dim]:
                    # å¦‚æœæœ‰æœªçŸ¥çš„typeï¼Œæš‚å½’ä¸ºunique
                    node_type = "unique"
                node_count[year][dim][node_type].append(node)
        
        # æ£€æŸ¥å¹¶è¡¥å……ç¼ºå¤±çš„èŠ‚ç‚¹
        # ä½¿ç”¨ max(existing_ids) æ¥é¿å… id å†²çª
        max_id = 0
        for node in nodes:
            try:
                # å°è¯•è§£æç±»ä¼¼ "2026_n12" æˆ– "n12" ä¸­çš„æ•°å­—
                parts = node["id"].split("n")
                if len(parts) > 1 and parts[-1].isdigit():
                    max_id = max(max_id, int(parts[-1]))
            except:
                pass
        node_id_counter = max_id + 1
        
        # è·Ÿè¸ªæ¯ä¸ªå¹´ä»½-ç»´åº¦å·²ä½¿ç”¨çš„å¤§å¸ˆ
        used_masters_by_year_dim = {}
        
        # å¿…é¡»å…ˆéå†ä¸€éç°æœ‰èŠ‚ç‚¹ï¼Œåˆå§‹åŒ– used_masters_by_year_dim
        # å¦åˆ™åé¢çš„è¡¥å……é€»è¾‘ä¼šè®¤ä¸ºæ²¡æœ‰å¤§å¸ˆè¢«ä½¿ç”¨
        for node in nodes:
            props = node.get("properties", {})
            year = props.get("time", "")
            dim = props.get("dimension", "")
            master = props.get("source_master", "")
            key = f"{year}-{dim}"
            if key not in used_masters_by_year_dim:
                used_masters_by_year_dim[key] = []
            if master and master not in used_masters_by_year_dim[key]:
                used_masters_by_year_dim[key].append(master)

        # ç¡®ä¿æ‰€æœ‰åŸå§‹èŠ‚ç‚¹éƒ½åŒ…å«åœ¨ç»“æœä¸­
        # ä¹‹å‰å¯èƒ½å› ä¸º node_count çš„åˆ†ç±»å¯¼è‡´éƒ¨åˆ†èŠ‚ç‚¹ä¸¢å¤±
        # æˆ‘ä»¬ä¸å†é‡ç½® nodes åˆ—è¡¨ï¼Œè€Œæ˜¯ç›´æ¥å‘å…¶ä¸­æ·»åŠ æ–°èŠ‚ç‚¹
        
        for year in target_years:
            year_total = 0
            for dim in required_dims:
                dim_nodes = node_count[year][dim]
                dim_total = sum(len(v) for v in dim_nodes.values())
                year_total += dim_total
                
                # æ”¶é›†è¯¥å¹´ä»½-ç»´åº¦å·²ä½¿ç”¨çš„å¤§å¸ˆ
                key = f"{year}-{dim}"
                if key not in used_masters_by_year_dim:
                    used_masters_by_year_dim[key] = []
                
                # æ¯ä¸ªç»´åº¦è‡³å°‘éœ€è¦1ä¸ªå…±è¯†èŠ‚ç‚¹
                if not dim_nodes.get("consensus"):
                    logger.info(f"{year}-{dim} ç¼ºå°‘å…±è¯†èŠ‚ç‚¹ï¼Œæ­£åœ¨è¡¥å……...")
                    
                    desc, master = self._extract_rich_description(
                        preprocessed_reports, dim, year, 
                        exclude_masters=used_masters_by_year_dim[key]
                    )
                    
                    if not desc:
                        desc = f"æ ¹æ®{dim_names[dim]}æ¨æ¼”ï¼Œ{year}æ•´ä½“è¶‹åŠ¿å¹³ç¨³ï¼Œå»ºè®®ä¿æŒç°çŠ¶ï¼Œé™å¾…æ—¶æœºã€‚"
                        master = "å¤šä½å¤§å¸ˆ"
                    
                    title = self._extract_node_title(desc, dim, "consensus")
                    source_quote = desc[:800] if len(desc) > 800 else desc
                    
                    new_node = {
                        "id": f"n{node_id_counter}",
                        "properties": {
                            "name": title,
                            "time": year,
                            "description": f"ã€æ¥æºï¼šä¼—å¸ˆå…±è¯†ã€‘å‚è€ƒå¤§å¸ˆè§‚ç‚¹ï¼šã€Œ{source_quote}ã€\n\nç»¼åˆå¤šä½å¤§å¸ˆå…±è¯†ï¼š{desc}",
                            "master_name": "ä¼—å¸ˆå…±è¯†",
                            "source_quote": source_quote,
                            "source_master": master,
                            "school_source": "ç»¼åˆæ¨æ¼”",
                            "type": "consensus",
                            "impact": random.randint(6, 9),
                            "dimension": dim
                        }
                    }
                    nodes.append(new_node)
                    node_id_counter += 1
                    dim_total += 1
                    year_total += 1
                
                # å¦‚æœè¯¥ç»´åº¦æ€»èŠ‚ç‚¹æ•°å¤ªå°‘ï¼ˆ<3ï¼‰ï¼Œå°è¯•è¡¥å……ä¸€äº› unique èŠ‚ç‚¹
                # è™½ç„¶ç”¨æˆ·è¯´ä¸é™æ•°é‡ï¼Œä½†å¦‚æœå¤ªå°‘ï¼ˆæ¯”å¦‚åªæœ‰1ä¸ªå…±è¯†ï¼‰ï¼ŒUIä¼šå¾ˆéš¾çœ‹
                if dim_total < 3:
                    needed = 3 - dim_total
                    logger.info(f"{year}-{dim} èŠ‚ç‚¹è¿‡å°‘({dim_total})ï¼Œè¡¥å…… {needed} ä¸ªç‹¬ç‰¹è§‚ç‚¹")
                    
                    # å°è¯•æå–æ›´å¤š unique èŠ‚ç‚¹
                    all_candidates = self._extract_all_relevant_paragraphs(preprocessed_reports, dim, year)
                    random.shuffle(all_candidates)
                    
                    added = 0
                    for para, master_name in all_candidates:
                        if added >= needed:
                            break
                        # å…è®¸å¤§å¸ˆé‡å¤ï¼Œåªè¦å†…å®¹ä¸åŒ
                        # ä½†å¦‚æœå¤§å¸ˆå·²ç»ä½œä¸ºå…±è¯†æ¥æºï¼Œè¿˜æ˜¯å°½é‡é¿å¼€ï¼Œé™¤éæ²¡å¾—é€‰
                        if master_name in used_masters_by_year_dim[key]:
                            # å¦‚æœå€™é€‰è€…å¤ªå°‘ï¼Œå…è®¸å¤ç”¨å¤§å¸ˆ
                            if len(all_candidates) > 5:
                                continue
                            
                        title = self._extract_node_title(para, dim, "unique")
                        source_quote = para[:800] if len(para) > 800 else para
                        
                        # æ£€æŸ¥å†…å®¹æ˜¯å¦é‡å¤
                        if any(source_quote in n.get("properties", {}).get("source_quote", "") for n in nodes):
                            continue
                        
                        new_node = {
                            "id": f"n{node_id_counter}",
                            "properties": {
                                "name": title,
                                "time": year,
                                "description": f"ã€æ¥æºï¼š{master_name}ã€‘åŸæ–‡ï¼šã€Œ{source_quote}ã€\n\n{master_name}ç‹¬ç‰¹è§è§£ï¼š{para}",
                                "master_name": master_name,
                                "source_quote": source_quote,
                                "source_master": master_name,
                                "school_source": "ç»¼åˆæ¨æ¼”",
                                "type": "unique",
                                "impact": random.randint(5, 8),
                                "dimension": dim
                            }
                        }
                        nodes.append(new_node)
                        node_id_counter += 1
                        used_masters_by_year_dim[key].append(master_name)
                        added += 1
                        dim_total += 1
                        year_total += 1

            # æ£€æŸ¥æ¯å¹´æ€»æ•°æ˜¯å¦è¾¾åˆ°20ä¸ªï¼ˆåŸºç¡€ä¿éšœï¼‰
            if year_total < 20:
                logger.warning(f"{year} å¹´èŠ‚ç‚¹æ€»æ•°ä¸è¶³: ç°æœ‰{year_total}ä¸ªï¼Œç›®æ ‡20ä¸ª")
        
        logger.info(f"èŠ‚ç‚¹è¡¥å……å®Œæˆï¼Œå½“å‰æ€»èŠ‚ç‚¹æ•°: {len(nodes)}")
        return nodes
